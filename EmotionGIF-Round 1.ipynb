{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from random import random\n",
    "import emoji\n",
    "from tqdm import notebook\n",
    "def tqdm(x, **kargs):\n",
    "    return notebook.tqdm(x, leave=False, **kargs)\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from os import listdir\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:24:50.501645 140553350014784 file_utils.py:39] PyTorch version 1.5.0 available.\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import MultiLabelClassificationModel, ClassificationModel\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.language_modeling import LanguageModelingModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>reply</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>we can all agree that any song by Niall Horan .</td>\n",
       "      <td>oui oui</td>\n",
       "      <td>[yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Will you be install  # ScottyFromMarketing  ' ...</td>\n",
       "      <td></td>\n",
       "      <td>[no]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Growing up my mum would call me a Nigga despit...</td>\n",
       "      <td>And he joins in ?  ?  ?  Pour some hot grits o...</td>\n",
       "      <td>[smh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Rest your head on my chest when the world feel...</td>\n",
       "      <td>ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚</td>\n",
       "      <td>[wink]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Imagine Will Hernandez and Wills both doing a ...</td>\n",
       "      <td></td>\n",
       "      <td>[yes]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx                                               text  \\\n",
       "0    0   we can all agree that any song by Niall Horan .    \n",
       "1    1  Will you be install  # ScottyFromMarketing  ' ...   \n",
       "2    2  Growing up my mum would call me a Nigga despit...   \n",
       "3    3  Rest your head on my chest when the world feel...   \n",
       "4    4  Imagine Will Hernandez and Wills both doing a ...   \n",
       "\n",
       "                                               reply categories  \n",
       "0                                            oui oui      [yes]  \n",
       "1                                                          [no]  \n",
       "2  And he joins in ?  ?  ?  Pour some hot grits o...      [smh]  \n",
       "3                                              ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚ðŸ˜‚     [wink]  \n",
       "4                                                         [yes]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_json('./source/train_gold.json', lines=True)\n",
    "df = pd.read_json('./preprocessed/preprocess_train.json', lines=True)\n",
    "df_new = df.copy()\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_type = pd.read_json('./source/categories.json', lines=True)\n",
    "categories_mapping = {v[0]: k for k, v in categories_type.to_dict('list').items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drop your cash app ,  use hashtag  # BailoutHu...</td>\n",
       "      <td>$ tyratomaro  # BailoutHumans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>After interviewing with a few incredible peopl...</td>\n",
       "      <td>CONGRATS !  !  !  !  !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I know GTC festival not happening next month b...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lordy ,  my daughter just said ,    \"  I wonde...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THE UNEMPLOYMENT CLAIM SYSTEM SUCKS SO MUCH DICK</td>\n",
       "      <td>Watching everyone else get their weekly unempl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Drop your cash app ,  use hashtag  # BailoutHu...   \n",
       "1  After interviewing with a few incredible peopl...   \n",
       "2  I know GTC festival not happening next month b...   \n",
       "3  Lordy ,  my daughter just said ,    \"  I wonde...   \n",
       "4   THE UNEMPLOYMENT CLAIM SYSTEM SUCKS SO MUCH DICK   \n",
       "\n",
       "                                               reply  \n",
       "0                      $ tyratomaro  # BailoutHumans  \n",
       "1                            CONGRATS !  !  !  !  !   \n",
       "2                                                     \n",
       "3                                                     \n",
       "4  Watching everyone else get their weekly unempl...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev_ori = pd.read_json('./source/dev_unlabeled.json', lines=True)\n",
    "df_dev = pd.read_json('./preprocessed/preprocess_dev.json', lines=True)\n",
    "df_dev_result = df_dev.copy()[['text', 'reply']]\n",
    "df_dev_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ Youngdeji  -   I think if uzi and carti dro...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For the third year in a row we are discussing ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dababy album sounds like it was made for nigga...</td>\n",
       "      <td>That  '  s why you buy it .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Majority of Indians do not watch any sport oth...</td>\n",
       "      <td>@ ZairaWasimmm got a great story because of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>everybody is just now listen to  @ madisonbeer...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0   @ Youngdeji  -   I think if uzi and carti dro...   \n",
       "1  For the third year in a row we are discussing ...   \n",
       "2  dababy album sounds like it was made for nigga...   \n",
       "3  Majority of Indians do not watch any sport oth...   \n",
       "4  everybody is just now listen to  @ madisonbeer...   \n",
       "\n",
       "                                               reply  \n",
       "0                                                     \n",
       "1                                                     \n",
       "2                       That  '  s why you buy it .   \n",
       "3   @ ZairaWasimmm got a great story because of t...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_ori = pd.read_json('./source/test_unlabeled.json', lines=True)\n",
    "df_test = pd.read_json('./preprocessed/preprocess_test.json', lines=True)\n",
    "df_test_result = df_test.copy()[['text', 'reply']]\n",
    "df_test_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use multi-hot encoding and change column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlb = MultiLabelBinarizer()\n",
    "multi_hot = mlb.fit_transform(df_new['categories'].values)\n",
    "multi_hot_list = [list(_) for _ in multi_hot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new['labels'] = multi_hot_list\n",
    "df_new = df_new[['text', 'reply', 'labels']]\n",
    "df_new.columns = ['text_a', 'text_b', 'labels']\n",
    "df_dev_result.columns = ['text_a', 'text_b']\n",
    "df_test_result.columns = ['text_a', 'text_b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:25:08.534508 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at cache/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0625 05:25:08.535140 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at cache/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0625 05:25:09.534883 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at cache/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0625 05:25:09.535674 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 05:25:10.701831 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at cache/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0625 05:25:17.851607 140553350014784 modeling_utils.py:741] Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "I0625 05:25:19.621090 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304bd9c4b061489c856a56d1895dbfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "077627d4e4f2492d8e68b5dda97f6b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13261.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:25:24.273219 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/roberta_cached_lm_110_training.txt\n",
      "I0625 05:25:24.305407 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f50de279dd7b4d50a82ee6fbef95bf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c0715155d74afba78160ec614624d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.962883"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:28:31.791547 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/checkpoint-829-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.569290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:28:32.055452 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/checkpoint-829-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa35f54ab7a4a6faa003e4162a22c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.113355"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:31:40.137413 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/checkpoint-1658-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.814610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:31:40.382909 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/checkpoint-1658-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e58da1bd98247499d96cd70e744c0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.130361"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:32:58.593721 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.870584"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:32:58.836330 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.749161"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:34:49.710997 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/checkpoint-2487-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.446024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:34:49.957314 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/checkpoint-2487-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4a4db3ffd14cadbe8c940ace7d182d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.660899"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:37:58.740855 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/checkpoint-3316-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.767858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:37:58.987230 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/checkpoint-3316-epoch-4/pytorch_model.bin\n",
      "I0625 05:37:59.496157 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:37:59.741293 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training1/pytorch_model.bin\n",
      "I0625 05:37:59.846119 140553350014784 language_modeling_model.py:402]  Training of roberta model complete. Saved to round1_blend_model/round1_LM/outputs_roberta_LM_training1/.\n",
      "I0625 05:37:59.858040 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_roberta_LM_training1/config.json\n",
      "I0625 05:37:59.858816 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 05:37:59.859320 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_roberta_LM_training1/pytorch_model.bin\n",
      "I0625 05:38:02.665916 140553350014784 modeling_utils.py:741] Weights of RobertaForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I0625 05:38:02.666437 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "I0625 05:38:02.667652 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_roberta_LM_training1/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_LM/outputs_roberta_LM_training1/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 05:38:02.668228 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_roberta_LM_training1/added_tokens.json. We won't load it.\n",
      "I0625 05:38:02.669411 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training1/vocab.json\n",
      "I0625 05:38:02.669752 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training1/merges.txt\n",
      "I0625 05:38:02.670065 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 05:38:02.670361 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training1/special_tokens_map.json\n",
      "I0625 05:38:02.670721 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training1/tokenizer_config.json\n",
      "I0625 05:38:02.868544 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346570bc40934e8fae62c8a930b63d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d163fcce34495897255e5eca5fb76b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:38:09.338501 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a89db5f0561c41b78a95f76d81b35e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.114284"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:43:41.473705 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-2000/config.json\n",
      "I0625 05:43:41.714225 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-2000/pytorch_model.bin\n",
      "I0625 05:43:42.288778 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:43:42.524587 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9943cf19de044b4dbd764475bc245d3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.125076"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:49:14.785210 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-4000/config.json\n",
      "I0625 05:49:15.022337 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-4000/pytorch_model.bin\n",
      "I0625 05:49:15.589354 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:49:15.824059 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af140b7d5c4b48dab2c4d8b402ba1588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.095449"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:54:48.163999 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-6000/config.json\n",
      "I0625 05:54:48.396509 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-6000/pytorch_model.bin\n",
      "I0625 05:54:48.960534 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 05:54:49.194231 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d23a3a49d14450bb66a8f3938bd675f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.102314"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:00:21.496441 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-8000/config.json\n",
      "I0625 06:00:21.727283 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-8000/pytorch_model.bin\n",
      "I0625 06:00:22.288019 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:00:22.520133 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 06:00:23.016297 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:00:23.248872 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training1/pytorch_model.bin\n",
      "I0625 06:00:23.351175 140553350014784 classification_model.py:279]  Training of roberta model complete. Saved to round1_blend_model/round1_MLR/outputs_roberta_LM_training1/.\n",
      "I0625 06:00:25.266054 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at cache/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0625 06:00:25.267689 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at cache/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0625 06:00:26.289146 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at cache/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0625 06:00:26.291658 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 06:00:26.362158 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at cache/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0625 06:00:30.021454 140553350014784 modeling_utils.py:741] Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "I0625 06:00:30.087695 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578f8d2cb2f14234b1fa48d27f1ae33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82654d532a14ccdb68be5d4767e7b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13261.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:00:34.556588 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/roberta_cached_lm_110_training.txt\n",
      "I0625 06:00:34.588813 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726a2b35fbf9434ab25f3a0662cc90e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80015e7c858846d184755620c2793637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.164397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:03:42.237235 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/checkpoint-829-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.080522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:03:42.495242 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/checkpoint-829-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a20b154c63641e49f983412fa7e2f3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.183931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:06:50.760699 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/checkpoint-1658-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.865083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:06:50.993956 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/checkpoint-1658-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c8a9aa89474025b03d26ce12e1da8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.121243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:08:09.023811 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.992506"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:08:09.256692 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.183822"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:09:59.986418 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/checkpoint-2487-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.723953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:10:00.219794 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/checkpoint-2487-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e519a8487ae463fa72f1c683943b207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.023238"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:14:27.085493 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/checkpoint-3316-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.819918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:14:27.321268 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/checkpoint-3316-epoch-4/pytorch_model.bin\n",
      "I0625 06:14:27.820841 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:14:28.054495 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training2/pytorch_model.bin\n",
      "I0625 06:14:28.164034 140553350014784 language_modeling_model.py:402]  Training of roberta model complete. Saved to round1_blend_model/round1_LM/outputs_roberta_LM_training2/.\n",
      "I0625 06:14:28.175446 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_roberta_LM_training2/config.json\n",
      "I0625 06:14:28.176171 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 06:14:28.176653 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_roberta_LM_training2/pytorch_model.bin\n",
      "I0625 06:14:30.804032 140553350014784 modeling_utils.py:741] Weights of RobertaForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I0625 06:14:30.804522 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "I0625 06:14:30.805341 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_roberta_LM_training2/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_LM/outputs_roberta_LM_training2/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 06:14:30.805763 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_roberta_LM_training2/added_tokens.json. We won't load it.\n",
      "I0625 06:14:30.806151 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training2/vocab.json\n",
      "I0625 06:14:30.806427 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training2/merges.txt\n",
      "I0625 06:14:30.806681 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 06:14:30.806946 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training2/special_tokens_map.json\n",
      "I0625 06:14:30.807195 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training2/tokenizer_config.json\n",
      "I0625 06:14:31.005445 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b585ca19fd84b8fbf48405cf35adc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2027c313c8164a629c4c4de8e59dd3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:14:37.216614 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c68ad9f9cb4665bd7f7a540b154b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.114304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:20:11.628825 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 0.114589"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:20:11.869045 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-2000/pytorch_model.bin\n",
      "I0625 06:20:12.449466 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:20:12.689627 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fee0bf4d46471892a1df3448a913ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.112159"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:25:45.898728 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-4000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 0.116609"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:25:46.140665 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-4000/pytorch_model.bin\n",
      "I0625 06:25:46.723787 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:25:46.970948 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227b08c0925544e8b7e26f50d46623fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.098921"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:31:19.835867 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-6000/config.json\n",
      "I0625 06:31:20.081577 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-6000/pytorch_model.bin\n",
      "I0625 06:31:20.671411 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:31:20.918949 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe18b3c1dcf45f7a7b86d31fcd6b980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.112554"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:36:53.435339 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-8000/config.json\n",
      "I0625 06:36:53.678670 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-8000/pytorch_model.bin\n",
      "I0625 06:36:54.265030 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:36:54.505392 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 06:36:55.019823 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:36:55.262822 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training2/pytorch_model.bin\n",
      "I0625 06:36:55.371542 140553350014784 classification_model.py:279]  Training of roberta model complete. Saved to round1_blend_model/round1_MLR/outputs_roberta_LM_training2/.\n",
      "I0625 06:36:57.226920 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at cache/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0625 06:36:57.228520 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at cache/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0625 06:36:58.210359 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at cache/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0625 06:36:58.212977 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 06:36:58.320774 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at cache/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0625 06:37:01.924525 140553350014784 modeling_utils.py:741] Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "I0625 06:37:01.992744 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d40a3a814a24157a8fbd3bbe41695de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1089a380d521402fabd2f2942b992999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13261.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:37:06.549438 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/roberta_cached_lm_110_training.txt\n",
      "I0625 06:37:06.582366 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d08aa1c4d5943ec8fa42c064f3e45d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644be682f21f4f63aabd8e28fbafd666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.326830"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:40:14.271893 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/checkpoint-829-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.748606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:40:14.536610 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/checkpoint-829-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd95bc851c04009afd9afd68786d478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.666608"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:43:22.811449 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/checkpoint-1658-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.023950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:43:23.051504 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/checkpoint-1658-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c63cc4fe494f14aafd5e6a06d456f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.941158"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:44:41.091337 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.832510"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:44:41.331795 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.158802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:46:32.090886 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/checkpoint-2487-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.791906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:46:32.330514 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/checkpoint-2487-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaba92b3b2a459db2de111c59e635e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.975289"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:49:40.598639 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/checkpoint-3316-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.684925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:49:40.840871 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/checkpoint-3316-epoch-4/pytorch_model.bin\n",
      "I0625 06:49:41.346680 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:49:41.585749 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training3/pytorch_model.bin\n",
      "I0625 06:49:41.697777 140553350014784 language_modeling_model.py:402]  Training of roberta model complete. Saved to round1_blend_model/round1_LM/outputs_roberta_LM_training3/.\n",
      "I0625 06:49:41.709271 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_roberta_LM_training3/config.json\n",
      "I0625 06:49:41.710001 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 06:49:41.710438 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_roberta_LM_training3/pytorch_model.bin\n",
      "I0625 06:49:44.328319 140553350014784 modeling_utils.py:741] Weights of RobertaForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I0625 06:49:44.328857 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "I0625 06:49:44.330041 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_roberta_LM_training3/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_LM/outputs_roberta_LM_training3/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 06:49:44.331088 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_roberta_LM_training3/added_tokens.json. We won't load it.\n",
      "I0625 06:49:44.331742 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training3/vocab.json\n",
      "I0625 06:49:44.332026 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training3/merges.txt\n",
      "I0625 06:49:44.332385 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 06:49:44.332683 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training3/special_tokens_map.json\n",
      "I0625 06:49:44.332983 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training3/tokenizer_config.json\n",
      "I0625 06:49:44.533619 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ceed1f12faa482bab0032bf0776c218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3cfc056d8464631992b4d3cfb51bfab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:49:50.710671 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7cbe4198e854cd090aec594592c983b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.115064"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:55:22.873230 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 0.154612"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:55:23.111440 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-2000/pytorch_model.bin\n",
      "I0625 06:55:23.686391 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 06:55:23.919241 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60ab848cba14afe907856b8795c3767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.114842"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:01:06.254955 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-4000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 0.101807"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:01:06.490992 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-4000/pytorch_model.bin\n",
      "I0625 07:01:07.065570 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:01:07.300669 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b1d7c3afe7f4beb86c46e67b154e801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.117740"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:07:54.036285 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-6000/config.json\n",
      "I0625 07:07:54.304012 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-6000/pytorch_model.bin\n",
      "I0625 07:07:54.962166 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:07:55.224094 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598982ed52b841afaa021db000c144c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.083248"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:13:40.675451 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-8000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 0.108901"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:13:40.913832 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-8000/pytorch_model.bin\n",
      "I0625 07:13:41.491872 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:13:41.725772 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 07:13:42.231523 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:13:42.467396 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training3/pytorch_model.bin\n",
      "I0625 07:13:42.579168 140553350014784 classification_model.py:279]  Training of roberta model complete. Saved to round1_blend_model/round1_MLR/outputs_roberta_LM_training3/.\n",
      "I0625 07:13:44.481791 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at cache/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0625 07:13:44.483508 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at cache/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0625 07:13:45.449205 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at cache/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0625 07:13:45.451739 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 07:13:45.761868 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at cache/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0625 07:13:49.320474 140553350014784 modeling_utils.py:741] Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "I0625 07:13:49.384418 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf75f4134304dc98cc58eccbd9644ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83fda864feb48f88af91a4522f76690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13261.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:13:53.668985 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/roberta_cached_lm_110_training.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:13:53.701738 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3402a32c76b84829b16b5c05edaa1d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb730966f6b45b4985d2890ebd09f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.673570"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:17:01.378839 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/checkpoint-829-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.482518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:17:01.662336 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/checkpoint-829-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d6b305830141fe8c28f05c0f098928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.922761"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:20:10.139467 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/checkpoint-1658-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.383377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:20:10.417927 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/checkpoint-1658-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b604dc09f34f40ab91267fb6981dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.847494"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:21:28.587459 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.888852"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:21:28.873519 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.643356"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:23:20.969508 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/checkpoint-2487-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.630570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:23:21.249214 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/checkpoint-2487-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cffa0ab1bd845bbb2b8479cc21fd007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.890281"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:26:37.637382 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/checkpoint-3316-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.670182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:26:38.161147 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/checkpoint-3316-epoch-4/pytorch_model.bin\n",
      "I0625 07:26:39.284713 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:26:39.803427 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training4/pytorch_model.bin\n",
      "I0625 07:26:39.990931 140553350014784 language_modeling_model.py:402]  Training of roberta model complete. Saved to round1_blend_model/round1_LM/outputs_roberta_LM_training4/.\n",
      "I0625 07:26:40.011576 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_roberta_LM_training4/config.json\n",
      "I0625 07:26:40.013036 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 07:26:40.014268 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_roberta_LM_training4/pytorch_model.bin\n",
      "I0625 07:26:43.150569 140553350014784 modeling_utils.py:741] Weights of RobertaForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I0625 07:26:43.152119 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "I0625 07:26:43.154585 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_roberta_LM_training4/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_LM/outputs_roberta_LM_training4/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 07:26:43.156039 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_roberta_LM_training4/added_tokens.json. We won't load it.\n",
      "I0625 07:26:43.157263 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training4/vocab.json\n",
      "I0625 07:26:43.158303 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training4/merges.txt\n",
      "I0625 07:26:43.159287 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 07:26:43.160252 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training4/special_tokens_map.json\n",
      "I0625 07:26:43.161210 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training4/tokenizer_config.json\n",
      "I0625 07:26:43.406645 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df858d82aaa49be937b45ba22aabf83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe3bf12c459243c5babb7593e99431e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:26:53.223182 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d156d4ef8d3c42178c51750a08fa7b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.135580"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:32:27.202776 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-2000/config.json\n",
      "I0625 07:32:27.484516 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-2000/pytorch_model.bin\n",
      "I0625 07:32:28.132241 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:32:28.387031 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babe30a2ae054bafbf3e381581e3be78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.119555"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:38:03.316571 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-4000/config.json\n",
      "I0625 07:38:03.584488 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-4000/pytorch_model.bin\n",
      "I0625 07:38:04.136787 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:38:04.393097 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e51cc5b5e843489a8b1f26c0ab6d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.126936"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:43:39.657282 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-6000/config.json\n",
      "I0625 07:43:39.912432 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-6000/pytorch_model.bin\n",
      "I0625 07:43:40.453897 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:43:40.699167 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8388d6c55ae54b579c6296966c3f16ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.101071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:49:16.481188 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-8000/config.json\n",
      "I0625 07:49:18.765015 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-8000/pytorch_model.bin\n",
      "I0625 07:49:29.645061 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:49:32.261905 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 07:49:43.838168 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:49:48.247571 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training4/pytorch_model.bin\n",
      "I0625 07:49:48.312536 140553350014784 classification_model.py:279]  Training of roberta model complete. Saved to round1_blend_model/round1_MLR/outputs_roberta_LM_training4/.\n",
      "I0625 07:49:54.281982 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at cache/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0625 07:49:54.283745 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at cache/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0625 07:49:55.776034 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at cache/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0625 07:49:55.776758 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 07:49:56.078123 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at cache/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0625 07:50:09.735248 140553350014784 modeling_utils.py:741] Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "I0625 07:50:10.095935 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dee1777c00a422096b204b37c3fffd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68fc0e67131a433e937276777ec313d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13261.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:50:16.447592 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/roberta_cached_lm_110_training.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:50:16.657755 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b90920451dc48b7a3ef4a854e4acc36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce19c30fab6c46f6bd1ce46e6aead854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.067681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:53:58.756458 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/checkpoint-829-epoch-1/config.json\n",
      "I0625 07:54:02.088817 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/checkpoint-829-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125365d520fe496b8433bcff73e41476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.669096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:58:07.921395 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/checkpoint-1658-epoch-2/config.json\n",
      "I0625 07:58:11.144514 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/checkpoint-1658-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a8c754447f4a749b0d591f93bfc77f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.387699"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 07:59:48.817857 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/checkpoint-2000/config.json\n",
      "I0625 07:59:49.267788 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.915835"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:01:41.834303 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/checkpoint-2487-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.757025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:01:42.114147 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/checkpoint-2487-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa2a5ceeb134e2bb19ee9daa44c4a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.522035"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:04:50.592855 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/checkpoint-3316-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.285318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:04:50.870141 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/checkpoint-3316-epoch-4/pytorch_model.bin\n",
      "I0625 08:04:51.544414 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:04:51.825421 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training5/pytorch_model.bin\n",
      "I0625 08:04:51.865238 140553350014784 language_modeling_model.py:402]  Training of roberta model complete. Saved to round1_blend_model/round1_LM/outputs_roberta_LM_training5/.\n",
      "I0625 08:04:51.866953 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_roberta_LM_training5/config.json\n",
      "I0625 08:04:51.868741 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 08:04:51.869465 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_roberta_LM_training5/pytorch_model.bin\n",
      "I0625 08:04:55.697212 140553350014784 modeling_utils.py:741] Weights of RobertaForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I0625 08:04:55.698435 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "I0625 08:04:55.700222 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_roberta_LM_training5/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_LM/outputs_roberta_LM_training5/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 08:04:55.701149 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_roberta_LM_training5/added_tokens.json. We won't load it.\n",
      "I0625 08:04:55.701811 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training5/vocab.json\n",
      "I0625 08:04:55.702097 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training5/merges.txt\n",
      "I0625 08:04:55.702454 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 08:04:55.702860 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training5/special_tokens_map.json\n",
      "I0625 08:04:55.703166 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training5/tokenizer_config.json\n",
      "I0625 08:04:56.371191 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8a27cdbf1c4f8e9646d9fbc3fcc71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deeed8146f634610ac26e49d2b710f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:05:03.290954 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6c81e95f2644b9bfc450548707fcb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.108527"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:10:35.119913 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 0.143277"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:10:35.361126 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-2000/pytorch_model.bin\n",
      "I0625 08:10:35.864554 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:10:36.106656 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6754a7f70dd7493b832075f9ed54103d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.136674"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:16:08.140275 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-4000/config.json\n",
      "I0625 08:16:08.377977 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-4000/pytorch_model.bin\n",
      "I0625 08:16:08.954873 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:16:09.194377 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30219ab049784b6b9af66d0e86ac673a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.115868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:21:41.200240 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-6000/config.json\n",
      "I0625 08:21:41.439832 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-6000/pytorch_model.bin\n",
      "I0625 08:21:42.018385 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:21:42.257023 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3d718d76e34230be2a3e48562d2f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.103348"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:27:14.440842 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-8000/config.json\n",
      "I0625 08:27:14.679692 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-8000/pytorch_model.bin\n",
      "I0625 08:27:15.179928 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:27:15.418673 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 08:27:15.988947 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:27:16.229059 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training5/pytorch_model.bin\n",
      "I0625 08:27:16.269002 140553350014784 classification_model.py:279]  Training of roberta model complete. Saved to round1_blend_model/round1_MLR/outputs_roberta_LM_training5/.\n",
      "I0625 08:27:18.345358 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at cache/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0625 08:27:18.347244 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at cache/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0625 08:27:19.411450 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at cache/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0625 08:27:19.414014 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 08:27:19.465720 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at cache/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0625 08:27:26.473419 140553350014784 modeling_utils.py:741] Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "I0625 08:27:26.560080 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f171aabebabc45388937382f2f92bf26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac970ad4659a4ba2a3650d41ee3ce761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13261.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:27:30.876056 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/roberta_cached_lm_110_training.txt\n",
      "I0625 08:27:30.934590 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92227d2c4034427494f5fe98983afaf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa89d868499b454ebcc9c7e86501ff75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.672768"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:30:38.655346 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/checkpoint-829-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.178255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:30:38.934342 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/checkpoint-829-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c7f4b8535f458ea68544ce624a6a1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.998688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:33:47.370679 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/checkpoint-1658-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.883096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:33:47.646764 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/checkpoint-1658-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bdadddae86432eac237950b3f40607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.516732"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:35:05.888500 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.568478"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:35:06.162088 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.088593"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:36:56.991006 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/checkpoint-2487-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.766839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:36:57.263880 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/checkpoint-2487-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86db936c48eb4e8f903a3fb06ce1bcb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.790088"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:40:05.553657 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/checkpoint-3316-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.760440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:40:05.829640 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/checkpoint-3316-epoch-4/pytorch_model.bin\n",
      "I0625 08:40:06.496311 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:40:06.770631 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training6/pytorch_model.bin\n",
      "I0625 08:40:06.885639 140553350014784 language_modeling_model.py:402]  Training of roberta model complete. Saved to round1_blend_model/round1_LM/outputs_roberta_LM_training6/.\n",
      "I0625 08:40:06.896805 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_roberta_LM_training6/config.json\n",
      "I0625 08:40:06.897547 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 08:40:06.898077 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_roberta_LM_training6/pytorch_model.bin\n",
      "I0625 08:40:09.570506 140553350014784 modeling_utils.py:741] Weights of RobertaForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I0625 08:40:09.571144 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "I0625 08:40:09.572126 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_roberta_LM_training6/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_LM/outputs_roberta_LM_training6/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 08:40:09.572618 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_roberta_LM_training6/added_tokens.json. We won't load it.\n",
      "I0625 08:40:09.572936 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training6/vocab.json\n",
      "I0625 08:40:09.573209 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training6/merges.txt\n",
      "I0625 08:40:09.573460 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 08:40:09.573710 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training6/special_tokens_map.json\n",
      "I0625 08:40:09.573985 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training6/tokenizer_config.json\n",
      "I0625 08:40:09.711509 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c56684f1079b44c1886ad0eebf156d0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcffcfc05ef4626a80979fae1a5a294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:40:16.021068 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea1407252af4fa0914659e463274d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.098785"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:45:47.687156 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-2000/config.json\n",
      "I0625 08:45:47.926491 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-2000/pytorch_model.bin\n",
      "I0625 08:45:48.508738 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:45:48.748883 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213b01a61f2148a9b0a65b9fe2d05ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.121492"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:51:20.813324 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-4000/config.json\n",
      "I0625 08:51:21.052236 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-4000/pytorch_model.bin\n",
      "I0625 08:51:21.556857 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:51:21.797103 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dfb06cc4a24ef5915e4cbf7718ffa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.116938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:56:53.873266 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-6000/config.json\n",
      "I0625 08:56:54.111954 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-6000/pytorch_model.bin\n",
      "I0625 08:56:54.614992 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 08:56:54.853673 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10af6d5b05fc4cdbab4f0a208b947c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.113156"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:02:27.084634 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-8000/config.json\n",
      "I0625 09:02:27.321626 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-8000/pytorch_model.bin\n",
      "I0625 09:02:27.823510 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:02:28.062150 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 09:02:28.634342 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:02:28.872462 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training6/pytorch_model.bin\n",
      "I0625 09:02:28.912809 140553350014784 classification_model.py:279]  Training of roberta model complete. Saved to round1_blend_model/round1_MLR/outputs_roberta_LM_training6/.\n",
      "I0625 09:02:30.779096 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at cache/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0625 09:02:30.780880 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at cache/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0625 09:02:31.777253 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at cache/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0625 09:02:31.780766 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 09:02:32.134133 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at cache/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0625 09:02:35.774996 140553350014784 modeling_utils.py:741] Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "I0625 09:02:35.844111 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d94ceb9dea724d79a18c7eba9bebe096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3ffa62eec145ed8c1b4e89de44f627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13261.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:02:40.135933 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/roberta_cached_lm_110_training.txt\n",
      "I0625 09:02:40.170036 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baddc54d165d452f8f716ee5bf604c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa8e1818af142108b720e9d26e4dd8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.621936"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:05:47.780105 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/checkpoint-829-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.062628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:05:48.064784 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/checkpoint-829-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a303eecb53c644c0985c839fb46ec5d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.266309"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:08:56.621088 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/checkpoint-1658-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.003282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:08:56.902837 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/checkpoint-1658-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbda9f6e2ec144fe99743273767df0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.244143"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:10:14.920072 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.840171"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:10:15.200301 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.789269"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:12:06.075839 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/checkpoint-2487-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.503498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:12:06.357804 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/checkpoint-2487-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34932aabbe3048d58f512af0c9c6c849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.577499"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:15:14.618592 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/checkpoint-3316-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.711509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:15:14.900907 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/checkpoint-3316-epoch-4/pytorch_model.bin\n",
      "I0625 09:15:15.553563 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:15:15.837472 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training7/pytorch_model.bin\n",
      "I0625 09:15:15.877500 140553350014784 language_modeling_model.py:402]  Training of roberta model complete. Saved to round1_blend_model/round1_LM/outputs_roberta_LM_training7/.\n",
      "I0625 09:15:15.878353 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_roberta_LM_training7/config.json\n",
      "I0625 09:15:15.879062 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 09:15:15.879477 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_roberta_LM_training7/pytorch_model.bin\n",
      "I0625 09:15:18.549505 140553350014784 modeling_utils.py:741] Weights of RobertaForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I0625 09:15:18.549988 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "I0625 09:15:18.550807 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_roberta_LM_training7/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_LM/outputs_roberta_LM_training7/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 09:15:18.551290 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_roberta_LM_training7/added_tokens.json. We won't load it.\n",
      "I0625 09:15:18.551650 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training7/vocab.json\n",
      "I0625 09:15:18.551924 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training7/merges.txt\n",
      "I0625 09:15:18.552181 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 09:15:18.552424 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training7/special_tokens_map.json\n",
      "I0625 09:15:18.552735 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training7/tokenizer_config.json\n",
      "I0625 09:15:18.774778 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e30dd1973c4ae896363f99dd60e1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca92d76e2d342b68704e4903e8d24ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:15:25.050495 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5def2c45a22e44a79ed0480900fe3e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.164456"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:20:56.746821 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-2000/config.json\n",
      "I0625 09:20:56.981673 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-2000/pytorch_model.bin\n",
      "I0625 09:20:57.484922 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:20:57.719896 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f455926edc4bcea112b2bcb524a4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.119009"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:26:30.062142 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-4000/config.json\n",
      "I0625 09:26:30.295877 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-4000/pytorch_model.bin\n",
      "I0625 09:26:30.795925 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:26:31.029778 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21fe047a979d4ddd8ae630e9b45be62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.106008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:32:03.176951 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-6000/config.json\n",
      "I0625 09:32:03.411094 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-6000/pytorch_model.bin\n",
      "I0625 09:32:03.983870 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:32:04.219388 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34f4b4b90c7493b854549777bd844fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.096438"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:37:36.428897 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-8000/config.json\n",
      "I0625 09:37:36.663480 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-8000/pytorch_model.bin\n",
      "I0625 09:37:37.236173 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:37:37.470963 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 09:37:37.975592 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:37:38.213640 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training7/pytorch_model.bin\n",
      "I0625 09:37:38.325128 140553350014784 classification_model.py:279]  Training of roberta model complete. Saved to round1_blend_model/round1_MLR/outputs_roberta_LM_training7/.\n",
      "I0625 09:37:40.246248 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at cache/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0625 09:37:40.246672 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at cache/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0625 09:37:41.266753 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at cache/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0625 09:37:41.269318 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 09:37:41.594712 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at cache/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0625 09:37:45.220987 140553350014784 modeling_utils.py:741] Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "I0625 09:37:45.288833 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc8d6aa121a4c2085d0a3ac6b7063a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d98989a310c455e9a71af4ad7cb549c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13261.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:37:49.582536 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/roberta_cached_lm_110_training.txt\n",
      "I0625 09:37:49.616011 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d6b9a76c5747b78d58584afd416b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5246f1d9a61474f8497f43b00097612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.766203"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:40:57.251316 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/checkpoint-829-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.417781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:40:57.532216 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/checkpoint-829-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81523e3f410348068934031edcf25260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.854828"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:44:05.901350 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/checkpoint-1658-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.175598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:44:06.183884 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/checkpoint-1658-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ccf888259b43889f7c8063d8d74909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.947042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:45:24.380226 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.962857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:45:24.662266 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.794658"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:47:15.556111 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/checkpoint-2487-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.480054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:47:15.841112 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/checkpoint-2487-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aefd4cdc6af4facb79af92bc05ca7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.705920"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:50:24.278947 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/checkpoint-3316-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.865401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:50:24.562480 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/checkpoint-3316-epoch-4/pytorch_model.bin\n",
      "I0625 09:50:25.157584 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:50:25.511785 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training8/pytorch_model.bin\n",
      "I0625 09:50:25.552635 140553350014784 language_modeling_model.py:402]  Training of roberta model complete. Saved to round1_blend_model/round1_LM/outputs_roberta_LM_training8/.\n",
      "I0625 09:50:25.563870 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_roberta_LM_training8/config.json\n",
      "I0625 09:50:25.564594 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 09:50:25.565001 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_roberta_LM_training8/pytorch_model.bin\n",
      "I0625 09:50:28.249201 140553350014784 modeling_utils.py:741] Weights of RobertaForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I0625 09:50:28.249675 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "I0625 09:50:28.250505 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_roberta_LM_training8/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_LM/outputs_roberta_LM_training8/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 09:50:28.250933 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_roberta_LM_training8/added_tokens.json. We won't load it.\n",
      "I0625 09:50:28.251281 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training8/vocab.json\n",
      "I0625 09:50:28.251539 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training8/merges.txt\n",
      "I0625 09:50:28.251841 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 09:50:28.252084 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training8/special_tokens_map.json\n",
      "I0625 09:50:28.252330 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training8/tokenizer_config.json\n",
      "I0625 09:50:28.456660 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95ecd36456734047a5c9823130857ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4317c049d064f23a3a72af705eb76aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:50:34.815868 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668d751440664d46ab9491f4dbea08d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.162446"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:56:06.336951 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-2000/config.json\n",
      "I0625 09:56:06.576815 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-2000/pytorch_model.bin\n",
      "I0625 09:56:07.082067 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 09:56:07.322969 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2ccba07d15454e8312abcc3687a8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.121388"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:01:39.404955 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-4000/config.json\n",
      "I0625 10:01:39.643904 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-4000/pytorch_model.bin\n",
      "I0625 10:01:40.229169 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:01:40.469108 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b6732ee1e44910b91c8e43c1f2c69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.143673"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:07:12.612117 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-6000/config.json\n",
      "I0625 10:07:12.851938 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-6000/pytorch_model.bin\n",
      "I0625 10:07:13.586902 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:07:13.824460 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec9245d332464e469693bff93c783c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.102841"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:12:46.038896 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-8000/config.json\n",
      "I0625 10:12:46.288314 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-8000/pytorch_model.bin\n",
      "I0625 10:12:46.816003 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:12:47.066128 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 10:12:47.664593 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:12:47.906425 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training8/pytorch_model.bin\n",
      "I0625 10:12:47.947399 140553350014784 classification_model.py:279]  Training of roberta model complete. Saved to round1_blend_model/round1_MLR/outputs_roberta_LM_training8/.\n",
      "I0625 10:12:49.807158 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at cache/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
      "I0625 10:12:49.809131 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at cache/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0625 10:12:51.034525 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at cache/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0625 10:12:51.035108 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 10:12:51.361753 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at cache/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0625 10:12:54.993401 140553350014784 modeling_utils.py:741] Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "I0625 10:12:55.060868 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f1c77c54dc446aa88b2e5802474976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c7956629c69419fa94bf068a49c62b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13261.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:12:59.246147 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/roberta_cached_lm_110_training.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:12:59.279043 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1242bc070e4b9abe205ec56e40abbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9644d16fb28b4b7c84b7e0d97328cc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.521662"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:16:06.944693 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/checkpoint-829-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.296630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:16:07.228261 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/checkpoint-829-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9448d4bba9c41509154229642a9b60a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.750175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:19:15.618120 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/checkpoint-1658-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.395921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:19:15.901508 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/checkpoint-1658-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "869fd502a50e4af5a15dd4af1a3eb0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.318687"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:20:34.045316 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.893247"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:20:34.332272 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.991867"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:22:25.167652 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/checkpoint-2487-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.518224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:22:25.451971 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/checkpoint-2487-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0820dd9a47d04cc5a326ca19e205c5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.738084"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:25:33.833346 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/checkpoint-3316-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.181219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:25:34.115576 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/checkpoint-3316-epoch-4/pytorch_model.bin\n",
      "I0625 10:25:34.702205 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:25:34.985701 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training9/pytorch_model.bin\n",
      "I0625 10:25:35.104635 140553350014784 language_modeling_model.py:402]  Training of roberta model complete. Saved to round1_blend_model/round1_LM/outputs_roberta_LM_training9/.\n",
      "I0625 10:25:35.115564 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_roberta_LM_training9/config.json\n",
      "I0625 10:25:35.116259 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 10:25:35.116652 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_roberta_LM_training9/pytorch_model.bin\n",
      "I0625 10:25:37.778486 140553350014784 modeling_utils.py:741] Weights of RobertaForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I0625 10:25:37.778952 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "I0625 10:25:37.779748 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_roberta_LM_training9/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_LM/outputs_roberta_LM_training9/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 10:25:37.780185 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_roberta_LM_training9/added_tokens.json. We won't load it.\n",
      "I0625 10:25:37.780641 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training9/vocab.json\n",
      "I0625 10:25:37.780955 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training9/merges.txt\n",
      "I0625 10:25:37.781226 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 10:25:37.781488 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training9/special_tokens_map.json\n",
      "I0625 10:25:37.781757 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training9/tokenizer_config.json\n",
      "I0625 10:25:37.921387 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a0f8e6aa1a4bea8c1e7bd68579ec49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d53e915ed84d369ca947a9a86d4d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:25:44.200655 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f80842a743144df8c77c70a589763b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.155093"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:31:15.777207 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-2000/config.json\n",
      "I0625 10:31:16.016896 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-2000/pytorch_model.bin\n",
      "I0625 10:31:16.517961 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:31:16.758897 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0150b896a489435f9ed60b34ff92c67a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.095413"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:36:48.856139 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-4000/config.json\n",
      "I0625 10:36:49.109147 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-4000/pytorch_model.bin\n",
      "I0625 10:36:49.639459 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:36:49.895266 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e097010993f417e85fbb3bd45073dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.097613"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:42:22.123955 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-6000/config.json\n",
      "I0625 10:42:22.365811 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-6000/pytorch_model.bin\n",
      "I0625 10:42:22.874834 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:42:23.115572 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training9/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d598957ebddb4ff7aa49601285b63980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.130967"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "I0625 10:48:00.533407 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at cache/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
      "I0625 10:48:00.534268 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 10:48:00.627708 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/roberta-base-pytorch_model.bin from cache at cache/80b4a484eddeb259bec2f06a6f2f05d90934111628e0e1c09a33bd4a121358e1.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
      "I0625 10:48:04.268411 140553350014784 modeling_utils.py:741] Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
      "I0625 10:48:04.336959 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c09b2e3f1c94a6db0b802e5f2848477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9a9031cfb94d588333d6d1df09d572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=13261.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:48:08.482597 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/roberta_cached_lm_110_training.txt\n",
      "I0625 10:48:08.515005 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96d6e9302d7e439484943f8713c32f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f4340e66594fb0a83afaaafacd20ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.543099"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:51:16.223474 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/checkpoint-829-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.005537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:51:16.499375 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/checkpoint-829-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1fedc08ce984f4da07d632ab1a726cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.664025"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:54:24.783552 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/checkpoint-1658-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.741112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:54:25.059654 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/checkpoint-1658-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b56c653fd7743f2abe98bf85d08aa53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.691818"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:55:43.227184 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.632390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:55:43.503095 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.074071"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:57:34.464171 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/checkpoint-2487-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.966635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 10:57:34.741027 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/checkpoint-2487-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933d4e43729240ca8a2c6d28380be5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=829.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.372322"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:00:43.105585 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/checkpoint-3316-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.908341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:00:43.382154 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/checkpoint-3316-epoch-4/pytorch_model.bin\n",
      "I0625 11:00:43.970371 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:00:44.247084 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_roberta_LM_training10/pytorch_model.bin\n",
      "I0625 11:00:44.366534 140553350014784 language_modeling_model.py:402]  Training of roberta model complete. Saved to round1_blend_model/round1_LM/outputs_roberta_LM_training10/.\n",
      "I0625 11:00:44.377388 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_roberta_LM_training10/config.json\n",
      "I0625 11:00:44.378094 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 11:00:44.378581 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_roberta_LM_training10/pytorch_model.bin\n",
      "I0625 11:00:47.052960 140553350014784 modeling_utils.py:741] Weights of RobertaForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "I0625 11:00:47.053440 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in RobertaForMultiLabelSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "I0625 11:00:47.054247 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_roberta_LM_training10/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_LM/outputs_roberta_LM_training10/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 11:00:47.054771 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_roberta_LM_training10/added_tokens.json. We won't load it.\n",
      "I0625 11:00:47.055125 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training10/vocab.json\n",
      "I0625 11:00:47.055403 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training10/merges.txt\n",
      "I0625 11:00:47.055651 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 11:00:47.055900 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training10/special_tokens_map.json\n",
      "I0625 11:00:47.056209 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_roberta_LM_training10/tokenizer_config.json\n",
      "I0625 11:00:47.190600 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd0f8427efd841cf92081dced1e27a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891f32cbde3c4a0396ef969d8bb11a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:00:53.487918 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0b6d4df4d04e01aac736f7ecf9ffb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.135126"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:06:24.992859 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-2000/config.json\n",
      "I0625 11:06:25.231822 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-2000/pytorch_model.bin\n",
      "I0625 11:06:25.743679 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:06:25.984945 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e05c8e8eb6244a08bfe77d9b9c64f8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.124666"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:11:58.228652 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-4000/config.json\n",
      "I0625 11:11:58.471113 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-4000/pytorch_model.bin\n",
      "I0625 11:11:58.981655 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:11:59.225439 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075f1289ceb04e028305d70c0e7f7443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.116929"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:17:31.448415 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-6000/config.json\n",
      "I0625 11:17:31.690576 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-6000/pytorch_model.bin\n",
      "I0625 11:17:32.201751 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:17:32.443406 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3601222c2f194e6494af191ecb195834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.105679"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:23:04.703508 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-8000/config.json\n",
      "I0625 11:23:04.943995 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-8000/pytorch_model.bin\n",
      "I0625 11:23:05.454222 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:23:05.697383 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 11:23:06.286602 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 11:23:06.528885 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_roberta_LM_training10/pytorch_model.bin\n",
      "I0625 11:23:06.569630 140553350014784 classification_model.py:279]  Training of roberta model complete. Saved to round1_blend_model/round1_MLR/outputs_roberta_LM_training10/.\n"
     ]
    }
   ],
   "source": [
    "# Output data to train_file for training LM\n",
    "df_forLM = df_new.copy()\n",
    "df_forLM['text'] = ['<s> '] + df_new['text_a'] + [' </s></s> '] + df_new['text_b'] + [' <s>']\n",
    "df_forLM['text'].to_csv(r'LM/training.txt', header=None, index=None, sep=' ')\n",
    "for i in range(1, 11):\n",
    "    train_lm_args = {\n",
    "        \"output_dir\": \"round1_blend_model/round1_LM/outputs_roberta_LM_training{}/\".format(i),\n",
    "        \"cache_dir\": \"cache/\",\n",
    "        \"best_model_dir\": \"LM/outputs/best_model/\",\n",
    "\n",
    "        \"fp16\": False,\n",
    "        \"fp16_opt_level\": \"O1\",\n",
    "        \"max_seq_length\": 113,\n",
    "        \"train_batch_size\": 16,\n",
    "        \"eval_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 4e-5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"do_lower_case\": False,\n",
    "    }\n",
    "    model = LanguageModelingModel(\"roberta\", \"roberta-base\", args=train_lm_args)\n",
    "    model.train_model('LM/training.txt')\n",
    "    train_args = {\n",
    "        \"output_dir\": \"round1_blend_model/round1_MLR/outputs_roberta_LM_training{}/\".format(i),\n",
    "        \"cache_dir\": \"cache/\",\n",
    "        \"best_model_dir\": \"model_results_gpu/outputs/best_model/\",\n",
    "\n",
    "        \"fp16\": False,\n",
    "        \"fp16_opt_level\": \"O1\",\n",
    "        \"max_seq_length\": 113,\n",
    "        \"train_batch_size\": 16,\n",
    "        \"eval_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 4e-5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"do_lower_case\": False,\n",
    "\n",
    "        \"logging_steps\": 50,\n",
    "        \"evaluate_during_training\": False,\n",
    "        \"evaluate_during_training_steps\": 2000,\n",
    "        \"evaluate_during_training_verbose\": False,\n",
    "        \"use_cached_eval_features\": False,\n",
    "        \"save_eval_checkpoints\": False,\n",
    "        \"save_steps\": 2000,\n",
    "        \"no_cache\": True,\n",
    "        \"save_model_every_epoch\": True,\n",
    "        \"tensorboard_dir\": None,\n",
    "\n",
    "        \"overwrite_output_dir\": False,\n",
    "        \"reprocess_input_data\": True,\n",
    "\n",
    "        \"n_gpu\": 1,\n",
    "        \"silent\": False,\n",
    "        \"use_multiprocessing\": False,\n",
    "\n",
    "        \"wandb_project\": None,\n",
    "        \"wandb_kwargs\": {},\n",
    "\n",
    "        \"use_early_stopping\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0,\n",
    "        \"early_stopping_metric\": \"eval_loss\",\n",
    "        \"early_stopping_metric_minimize\": True,\n",
    "\n",
    "        \"manual_seed\": None,\n",
    "        \"encoding\": None,\n",
    "        \"config\": {}\n",
    "    }\n",
    "    model = MultiLabelClassificationModel('roberta', 'round1_blend_model/round1_LM/outputs_roberta_LM_training{}/'.format(i), num_labels=43, args=train_args)\n",
    "    model.train_model(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 cased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:08:36.781080 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0625 14:08:37.774063 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "I0625 14:08:37.776594 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 14:08:38.116988 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at cache/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I0625 14:08:40.225229 140553350014784 modeling_utils.py:741] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0625 14:08:40.225846 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "I0625 14:08:40.314922 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34f3696192e43f8bdf67dff5230ea94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6a6e421903b4e6383a31ca188cb0fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:08:42.832368 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/bert_cached_lm_111_training.txt\n",
      "I0625 14:08:42.862173 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe6619da2ff482a86b71c3b15139985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc74471c4b14c0ea9313b8b1afc897c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.496701"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:11:08.730382 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/checkpoint-727-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:11:08.954952 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/checkpoint-727-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef80b6ac3a2c47c1bbb7f3fbe202e831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.795886"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:13:35.711536 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/checkpoint-1454-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:13:35.917648 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/checkpoint-1454-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8de608cc6a6c4a36af4f37e24dd9ae43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.921541"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:15:26.376803 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.279674"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:15:26.583136 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.537090"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:16:03.493756 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/checkpoint-2181-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:16:03.701166 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/checkpoint-2181-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89a6af8627964f7299cc9b9383d919cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.001107"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:18:31.027910 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/checkpoint-2908-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:18:31.233938 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/checkpoint-2908-epoch-4/pytorch_model.bin\n",
      "I0625 14:18:31.652704 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:18:31.859230 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/pytorch_model.bin\n",
      "I0625 14:18:31.871785 140553350014784 language_modeling_model.py:402]  Training of bert model complete. Saved to round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/.\n",
      "I0625 14:18:31.872619 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/config.json\n",
      "I0625 14:18:31.873284 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 14:18:31.873692 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/pytorch_model.bin\n",
      "I0625 14:18:33.721044 140553350014784 modeling_utils.py:741] Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0625 14:18:33.721527 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "I0625 14:18:33.722344 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 14:18:33.722805 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/added_tokens.json. We won't load it.\n",
      "I0625 14:18:33.723131 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/vocab.txt\n",
      "I0625 14:18:33.723384 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 14:18:33.723691 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/special_tokens_map.json\n",
      "I0625 14:18:33.723939 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training1/tokenizer_config.json\n",
      "I0625 14:18:33.946176 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53efafadb0e423a814d8d404eb358ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841bafec7c504d42a798a9a18a8e549b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:18:43.623184 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a001ac2da5df482d9ede37900f584da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.107159"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:24:07.463306 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-2000/config.json\n",
      "I0625 14:24:07.688369 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-2000/pytorch_model.bin\n",
      "I0625 14:24:08.151671 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:24:08.376837 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d822671cd6434099f5f6cb689a4f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.128944"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:29:32.906754 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-4000/config.json\n",
      "I0625 14:29:33.132620 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-4000/pytorch_model.bin\n",
      "I0625 14:29:33.593121 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:29:33.816469 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cff7d92d4c4fceb605d865b8fe26e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.105190"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:34:58.344184 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-6000/config.json\n",
      "I0625 14:34:58.568492 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-6000/pytorch_model.bin\n",
      "I0625 14:34:59.034890 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:34:59.264178 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67a0be07edb24778b5d246d47b95a438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.092475"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:40:23.928241 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-8000/config.json\n",
      "I0625 14:40:24.152147 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-8000/pytorch_model.bin\n",
      "I0625 14:40:24.614813 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:40:24.840337 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 14:40:25.304398 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:40:25.529416 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/pytorch_model.bin\n",
      "I0625 14:40:25.542315 140553350014784 classification_model.py:279]  Training of bert model complete. Saved to round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/.\n",
      "I0625 14:40:26.483079 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0625 14:40:27.436005 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "I0625 14:40:27.436951 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 14:40:28.316360 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at cache/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I0625 14:40:30.408586 140553350014784 modeling_utils.py:741] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0625 14:40:30.409094 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "I0625 14:40:30.463095 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "929ff2dbf49f43d5af60f71bf525601b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e152d7c5334c5d8b5fadba639477fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:40:33.072169 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/bert_cached_lm_111_training.txt\n",
      "I0625 14:40:33.102050 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41eaf01e0e404bfab7911de8f198ebfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eed823f99da48b79026d0f860311569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.735750"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:42:59.856498 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/checkpoint-727-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:43:00.091364 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/checkpoint-727-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92e625f6b604b3dbca5e81f9f1e3c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 3.047831"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:45:27.408991 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/checkpoint-1454-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:45:27.638131 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/checkpoint-1454-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267671fd4cba41e88bf6cd5d7d40f2f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.971163"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:47:18.505849 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.741267"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:47:18.736115 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.055412"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:47:55.729151 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/checkpoint-2181-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:47:55.959145 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/checkpoint-2181-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd167e42b3a4e78a4ce689382c6e547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.390111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:50:23.353800 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/checkpoint-2908-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:50:23.582527 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/checkpoint-2908-epoch-4/pytorch_model.bin\n",
      "I0625 14:50:24.056863 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:50:24.287001 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/pytorch_model.bin\n",
      "I0625 14:50:24.299891 140553350014784 language_modeling_model.py:402]  Training of bert model complete. Saved to round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/.\n",
      "I0625 14:50:24.300724 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/config.json\n",
      "I0625 14:50:24.301340 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 14:50:24.301776 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/pytorch_model.bin\n",
      "I0625 14:50:26.163357 140553350014784 modeling_utils.py:741] Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0625 14:50:26.163841 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "I0625 14:50:26.164638 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 14:50:26.165074 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/added_tokens.json. We won't load it.\n",
      "I0625 14:50:26.165455 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/vocab.txt\n",
      "I0625 14:50:26.165718 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 14:50:26.165965 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/special_tokens_map.json\n",
      "I0625 14:50:26.166211 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training2/tokenizer_config.json\n",
      "I0625 14:50:26.410541 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0377d6f89a1467395ef8174514322fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22bc43124e3445e9243e7776330bbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:50:36.056656 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1674b8f6fadf4d118d273f5fdf96c8e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.117138"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:56:00.330212 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-2000/config.json\n",
      "I0625 14:56:00.558524 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-2000/pytorch_model.bin\n",
      "I0625 14:56:01.025948 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 14:56:01.254211 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0d21904ebd41039e9571f6046878ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.102201"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:01:26.515304 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-4000/config.json\n",
      "I0625 15:01:26.779499 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-4000/pytorch_model.bin\n",
      "I0625 15:01:27.329119 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:01:27.595392 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd20122dd4e74be0910f3e416fc08da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.081895"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:06:55.507439 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-6000/config.json\n",
      "I0625 15:06:55.773914 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-6000/pytorch_model.bin\n",
      "I0625 15:06:56.283848 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:06:56.526335 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "075986f359034de1b66728f233b5d376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.100186"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:12:24.648556 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-8000/config.json\n",
      "I0625 15:12:24.891155 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-8000/pytorch_model.bin\n",
      "I0625 15:12:25.402055 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:12:25.644984 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 15:12:26.143663 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:12:26.385355 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/pytorch_model.bin\n",
      "I0625 15:12:26.398265 140553350014784 classification_model.py:279]  Training of bert model complete. Saved to round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/.\n",
      "I0625 15:12:27.381554 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0625 15:12:28.396566 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "I0625 15:12:28.397296 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 15:12:29.724337 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at cache/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I0625 15:12:32.200333 140553350014784 modeling_utils.py:741] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0625 15:12:32.200854 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "I0625 15:12:32.257666 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13cb8740c1d438992e68975199f9bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c190568ea0441d82290ec871fb149e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:12:35.519913 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/bert_cached_lm_111_training.txt\n",
      "I0625 15:12:35.573632 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8232bf32724175a07d404f5c823807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46acdcf3a3e04251ba7a40f14841110b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.698967"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:15:04.131878 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/checkpoint-727-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:15:04.376870 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/checkpoint-727-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93c6d8e8038429ea91948af55bcc3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.215762"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:17:33.126424 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/checkpoint-1454-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:17:33.389815 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/checkpoint-1454-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a56200c1254771a867ae98e891e4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.028144"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:19:25.732245 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.237626"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:19:25.996453 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.406075"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:20:03.382269 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/checkpoint-2181-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:20:03.648824 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/checkpoint-2181-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a45c84e3fa84f9badf99f4f45f674fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.296248"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:22:32.723828 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/checkpoint-2908-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:22:32.982852 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/checkpoint-2908-epoch-4/pytorch_model.bin\n",
      "I0625 15:22:33.485629 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:22:33.730964 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/pytorch_model.bin\n",
      "I0625 15:22:33.744744 140553350014784 language_modeling_model.py:402]  Training of bert model complete. Saved to round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/.\n",
      "I0625 15:22:33.745772 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/config.json\n",
      "I0625 15:22:33.746410 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 15:22:33.746854 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/pytorch_model.bin\n",
      "I0625 15:22:35.762106 140553350014784 modeling_utils.py:741] Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0625 15:22:35.762617 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "I0625 15:22:35.763617 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 15:22:35.764087 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/added_tokens.json. We won't load it.\n",
      "I0625 15:22:35.764584 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/vocab.txt\n",
      "I0625 15:22:35.764884 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 15:22:35.765149 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/special_tokens_map.json\n",
      "I0625 15:22:35.765406 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training3/tokenizer_config.json\n",
      "I0625 15:22:36.030207 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4e6b7ebfe74f109d79674f783234ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac1073bd38441c18703aef4345351ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:22:46.489781 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6f97e77f99488186ede0391f313d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.124944"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:28:14.534270 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-2000/config.json\n",
      "I0625 15:28:14.843669 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-2000/pytorch_model.bin\n",
      "I0625 15:28:16.995519 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:28:19.740602 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91402d65010c416f91e740456a54c551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.097135"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:35:28.650478 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-4000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 0.142649"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:35:31.832027 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-4000/pytorch_model.bin\n",
      "I0625 15:35:39.504169 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:35:40.243003 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6878a8e6808c47dc86e2604b9020a9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.078643"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:41:08.107039 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-6000/config.json\n",
      "I0625 15:41:08.347835 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-6000/pytorch_model.bin\n",
      "I0625 15:41:08.834165 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:41:09.069422 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f062f0772f441069f296a70dc288f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.106732"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:46:33.645919 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-8000/config.json\n",
      "I0625 15:46:33.883532 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-8000/pytorch_model.bin\n",
      "I0625 15:46:34.372809 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:46:34.606843 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 15:46:35.079002 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:46:35.313045 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/pytorch_model.bin\n",
      "I0625 15:46:35.324826 140553350014784 classification_model.py:279]  Training of bert model complete. Saved to round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/.\n",
      "I0625 15:46:36.622045 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0625 15:46:37.694032 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "I0625 15:46:37.698317 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 15:46:38.014067 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at cache/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I0625 15:46:43.969099 140553350014784 modeling_utils.py:741] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0625 15:46:43.969875 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "I0625 15:46:44.079527 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb5a2c7f5f845599ec0e1bed6447ccb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e50654d6a214cf3abe19db87c7d2bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:46:49.247057 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/bert_cached_lm_111_training.txt\n",
      "I0625 15:46:49.334615 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26750047de6b457db11d1c8ce2ff34f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c973fc017fcd426e8f4e120393b1d19e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.482389"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:49:16.340468 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/checkpoint-727-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:49:16.575620 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/checkpoint-727-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7965aa72104fe0bcb1a88399105f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.691603"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:51:43.986239 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/checkpoint-1454-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:51:44.216290 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/checkpoint-1454-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d188b538d0d4651bffc4e1308399204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.179919"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:53:35.037468 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.345303"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:53:35.267731 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.302135"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:54:12.262346 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/checkpoint-2181-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:54:12.496072 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/checkpoint-2181-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0084a4d7814c403795faa51e83b3cfb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.954354"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:56:39.943487 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/checkpoint-2908-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:56:40.172683 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/checkpoint-2908-epoch-4/pytorch_model.bin\n",
      "I0625 15:56:40.638429 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:56:40.869476 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/pytorch_model.bin\n",
      "I0625 15:56:40.882303 140553350014784 language_modeling_model.py:402]  Training of bert model complete. Saved to round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/.\n",
      "I0625 15:56:40.883166 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/config.json\n",
      "I0625 15:56:40.883809 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 15:56:40.884206 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/pytorch_model.bin\n",
      "I0625 15:56:42.806580 140553350014784 modeling_utils.py:741] Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0625 15:56:42.807071 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "I0625 15:56:42.807868 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 15:56:42.808270 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/added_tokens.json. We won't load it.\n",
      "I0625 15:56:42.808643 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/vocab.txt\n",
      "I0625 15:56:42.808908 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 15:56:42.809220 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/special_tokens_map.json\n",
      "I0625 15:56:42.809466 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training4/tokenizer_config.json\n",
      "I0625 15:56:42.944801 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b260730f1c04df0b3bfe44c3840b1f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1bad3d0de24ca390d12517e93c2cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 15:56:52.849359 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e129cf051d47a996fe9e1344280dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.135545"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:02:17.167510 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-2000/config.json\n",
      "I0625 16:02:17.403306 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-2000/pytorch_model.bin\n",
      "I0625 16:02:17.875593 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:02:18.111133 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99b46f5afe24550b81b32230ce5bbbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.119186"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:07:42.748549 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-4000/config.json\n",
      "I0625 16:07:42.981167 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-4000/pytorch_model.bin\n",
      "I0625 16:07:43.452700 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:07:43.685282 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f600cf02e99543d08638a4f77dc2055b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.144304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:13:08.352888 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-6000/config.json\n",
      "I0625 16:13:08.581530 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-6000/pytorch_model.bin\n",
      "I0625 16:13:09.051542 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:13:09.281621 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255484d0eb734b698ab234d548a78da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.100410"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:18:33.824717 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-8000/config.json\n",
      "I0625 16:18:34.056849 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-8000/pytorch_model.bin\n",
      "I0625 16:18:34.529218 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:18:34.765672 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 16:18:35.240055 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:18:35.472954 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/pytorch_model.bin\n",
      "I0625 16:18:35.485951 140553350014784 classification_model.py:279]  Training of bert model complete. Saved to round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/.\n",
      "I0625 16:18:36.412690 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at cache/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "I0625 16:18:37.363655 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at cache/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "I0625 16:18:37.366348 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 16:18:37.442638 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/bert-base-cased-pytorch_model.bin from cache at cache/d8f11f061e407be64c4d5d7867ee61d1465263e24085cfa26abf183fdc830569.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "I0625 16:18:39.588254 140553350014784 modeling_utils.py:741] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0625 16:18:39.588817 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "I0625 16:18:39.642915 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "043a3255ed8940d090590aab27c938e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddbb4e2b2ca49aba6a455de7d81b005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11625.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:18:42.285689 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/bert_cached_lm_111_training.txt\n",
      "I0625 16:18:42.318076 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9372652002bb49d9b7c9b53bd6252be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "605e9b7f5c834854b893358c7755af51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.436880"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:21:09.187352 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/checkpoint-727-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:21:09.423599 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/checkpoint-727-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bbe66b11a7a4f84b1672f6caceb79a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.828680"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:23:36.831860 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/checkpoint-1454-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:23:37.061742 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/checkpoint-1454-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7c34b1527ed45fc88a2cc3e39d9f87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.240345"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:25:27.955584 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.871348"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:25:28.185584 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.078594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:26:05.339386 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/checkpoint-2181-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:26:05.572808 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/checkpoint-2181-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89839d00c3bb4106a895e03a45c6e29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=727.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.020852"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:28:33.220718 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/checkpoint-2908-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:28:33.450230 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/checkpoint-2908-epoch-4/pytorch_model.bin\n",
      "I0625 16:28:33.923400 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:28:34.153721 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/pytorch_model.bin\n",
      "I0625 16:28:34.166365 140553350014784 language_modeling_model.py:402]  Training of bert model complete. Saved to round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/.\n",
      "I0625 16:28:34.167265 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/config.json\n",
      "I0625 16:28:34.168111 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 16:28:34.168647 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/pytorch_model.bin\n",
      "I0625 16:28:36.033995 140553350014784 modeling_utils.py:741] Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0625 16:28:36.034449 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "I0625 16:28:36.035240 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 16:28:36.035666 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/added_tokens.json. We won't load it.\n",
      "I0625 16:28:36.036175 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/vocab.txt\n",
      "I0625 16:28:36.036489 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 16:28:36.036755 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/special_tokens_map.json\n",
      "I0625 16:28:36.037023 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_cased_LM_training5/tokenizer_config.json\n",
      "I0625 16:28:36.278371 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda6253a79df4a86ae7a4fbfcc2dd9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821125c224b64440bb88ca9392a584d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:28:45.977287 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43365b5b9ee9416bbfd4e7beb93e9431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.128041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:34:12.702527 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-2000/config.json\n",
      "I0625 16:34:12.946095 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-2000/pytorch_model.bin\n",
      "I0625 16:34:13.442955 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:34:13.686304 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f536817baa444d936589450a6ec7e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.162613"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:39:41.595041 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-4000/config.json\n",
      "I0625 16:39:41.839086 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-4000/pytorch_model.bin\n",
      "I0625 16:39:42.349608 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:39:42.593930 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1831d98c211437ba5299348beac6d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.110772"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:45:10.537927 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-6000/config.json\n",
      "I0625 16:45:10.781403 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-6000/pytorch_model.bin\n",
      "I0625 16:45:11.275535 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:45:11.519190 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d4173ef2e942308cf723e523803376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.088493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:50:39.792351 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-8000/config.json\n",
      "I0625 16:50:40.038933 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-8000/pytorch_model.bin\n",
      "I0625 16:50:40.549069 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:50:40.795881 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 16:50:41.298336 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:50:41.543707 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/pytorch_model.bin\n",
      "I0625 16:50:41.556727 140553350014784 classification_model.py:279]  Training of bert model complete. Saved to round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/.\n"
     ]
    }
   ],
   "source": [
    "df_forLM = df_new.copy()\n",
    "df_forLM['text'] = ['[CLS] '] + df_new['text_a'] + [' [SEP] '] + df_new['text_b'] + [' [SEP]']\n",
    "df_forLM['text'].to_csv(r'LM/training.txt', header=None, index=None, sep=' ')\n",
    "for i in range(1, 6):\n",
    "    train_lm_args = {\n",
    "        \"output_dir\": \"round1_blend_model/round1_LM/outputs_bert_cased_LM_training{}/\".format(i),\n",
    "        \"cache_dir\": \"cache/\",\n",
    "        \"best_model_dir\": \"LM/outputs/best_model/\",\n",
    "\n",
    "        \"fp16\": False,\n",
    "        \"fp16_opt_level\": \"O1\",\n",
    "        \"max_seq_length\": 113,\n",
    "        \"train_batch_size\": 16,\n",
    "        \"eval_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 4e-5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"do_lower_case\": False,\n",
    "    }\n",
    "    model = LanguageModelingModel(\"bert\", \"bert-base-cased\", args=train_lm_args)\n",
    "    model.train_model('LM/training.txt')\n",
    "    train_args = {\n",
    "        \"output_dir\": \"round1_blend_model/round1_MLR/outputs_bert_cased_LM_training{}/\".format(i),\n",
    "        \"cache_dir\": \"cache/\",\n",
    "        \"best_model_dir\": \"model_results_gpu/outputs/best_model/\",\n",
    "\n",
    "        \"fp16\": False,\n",
    "        \"fp16_opt_level\": \"O1\",\n",
    "        \"max_seq_length\": 113,\n",
    "        \"train_batch_size\": 16,\n",
    "        \"eval_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 4e-5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"do_lower_case\": False,\n",
    "\n",
    "        \"logging_steps\": 50,\n",
    "        \"evaluate_during_training\": False,\n",
    "        \"evaluate_during_training_steps\": 2000,\n",
    "        \"evaluate_during_training_verbose\": False,\n",
    "        \"use_cached_eval_features\": False,\n",
    "        \"save_eval_checkpoints\": False,\n",
    "        \"save_steps\": 2000,\n",
    "        \"no_cache\": True,\n",
    "        \"save_model_every_epoch\": True,\n",
    "        \"tensorboard_dir\": None,\n",
    "\n",
    "        \"overwrite_output_dir\": False,\n",
    "        \"reprocess_input_data\": True,\n",
    "\n",
    "        \"n_gpu\": 1,\n",
    "        \"silent\": False,\n",
    "        \"use_multiprocessing\": False,\n",
    "\n",
    "        \"wandb_project\": None,\n",
    "        \"wandb_kwargs\": {},\n",
    "\n",
    "        \"use_early_stopping\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0,\n",
    "        \"early_stopping_metric\": \"eval_loss\",\n",
    "        \"early_stopping_metric_minimize\": True,\n",
    "\n",
    "        \"manual_seed\": None,\n",
    "        \"encoding\": None,\n",
    "        \"config\": {}\n",
    "    }\n",
    "    model = MultiLabelClassificationModel('bert', 'round1_blend_model/round1_LM/outputs_bert_cased_LM_training{}/'.format(i), num_labels=43, args=train_args)\n",
    "    model.train_model(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:53:36.430842 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0625 16:53:37.378387 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0625 16:53:37.379137 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 16:53:37.676994 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at cache/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0625 16:53:43.794977 140553350014784 modeling_utils.py:741] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0625 16:53:43.795515 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "I0625 16:53:43.889105 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9b30335c2cd416c97007af47deea822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c458b9149a3940389d02b9e9dea15384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11016.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:53:47.771228 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/bert_cached_lm_111_training.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:53:47.799728 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef74834e7f5f4a41bef159b84ae659a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ef6d05904c4f4eaf5acf825a80cddc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.934939"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:56:09.469473 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/checkpoint-689-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:56:09.722162 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/checkpoint-689-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f38d5f745d455095bd4a01f7bc1c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.513891"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:58:42.304158 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/checkpoint-1378-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 16:58:47.226934 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/checkpoint-1378-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0283013ee1a34e29868004981fe090ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.215434"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:01:29.522278 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/checkpoint-2000/config.json\n",
      "I0625 17:01:35.423902 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.684476"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:02:09.419273 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/checkpoint-2067-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.168634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:02:14.930937 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/checkpoint-2067-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86d47db441a4efc9237f84317161560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.500209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:05:05.721834 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/checkpoint-2756-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.491249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:05:10.147734 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/checkpoint-2756-epoch-4/pytorch_model.bin\n",
      "I0625 17:05:18.750560 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:05:23.146698 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/pytorch_model.bin\n",
      "I0625 17:05:24.080669 140553350014784 language_modeling_model.py:402]  Training of bert model complete. Saved to round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/.\n",
      "I0625 17:05:24.122827 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/config.json\n",
      "I0625 17:05:24.135772 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 17:05:24.267637 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/pytorch_model.bin\n",
      "I0625 17:05:36.409716 140553350014784 modeling_utils.py:741] Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0625 17:05:36.494328 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "I0625 17:05:36.497268 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 17:05:36.497801 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/added_tokens.json. We won't load it.\n",
      "I0625 17:05:36.498507 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/vocab.txt\n",
      "I0625 17:05:36.498906 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 17:05:36.499179 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/special_tokens_map.json\n",
      "I0625 17:05:36.499434 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training1/tokenizer_config.json\n",
      "I0625 17:05:43.001529 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52b3b1f4e514204a8f69210a8709927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a62016de310648f8943e19db044749b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:05:57.294331 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b4283533b3b45e985875de56b452799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.111672"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:11:26.185006 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-2000/config.json\n",
      "I0625 17:11:26.553205 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-2000/pytorch_model.bin\n",
      "I0625 17:11:27.043165 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:11:27.278031 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff88c5129de490b8d03985c39b6571a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.139381"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:16:51.805258 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-4000/config.json\n",
      "I0625 17:16:52.040086 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-4000/pytorch_model.bin\n",
      "I0625 17:16:52.518455 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:16:52.753504 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41aff3ee3ec94344a297646777f3ba44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.102437"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:22:17.334093 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-6000/config.json\n",
      "I0625 17:22:17.567348 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-6000/pytorch_model.bin\n",
      "I0625 17:22:18.049657 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:22:18.306269 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3014f6387e28441d95b0d0741fceb833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.081799"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:27:43.007748 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-8000/config.json\n",
      "I0625 17:27:43.241453 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-8000/pytorch_model.bin\n",
      "I0625 17:27:43.722487 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:27:43.956855 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 17:27:44.435769 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:27:44.670990 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/pytorch_model.bin\n",
      "I0625 17:27:44.683988 140553350014784 classification_model.py:279]  Training of bert model complete. Saved to round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/.\n",
      "I0625 17:27:45.888215 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0625 17:27:46.920866 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0625 17:27:46.923664 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 17:27:47.268130 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at cache/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0625 17:27:52.636358 140553350014784 modeling_utils.py:741] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0625 17:27:52.636903 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "I0625 17:27:52.734226 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90406c93456f4a71a32185ecf7d8e6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09391ac1de2a4472a076df61be3ff47d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11016.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:27:56.302506 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/bert_cached_lm_111_training.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:27:56.345934 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e33e673698f4d3898002fc5de22143e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f0457f116a47a7b298318ccabfd487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.423787"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:30:16.816198 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/checkpoint-689-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:30:17.058593 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/checkpoint-689-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cac8d33897341ca8732083b8af48abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.714483"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:32:37.997725 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/checkpoint-1378-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:32:38.233255 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/checkpoint-1378-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfee49e6389474784b769135883718a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.288928"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:34:45.625306 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.615797"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:34:45.863101 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.299904"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:34:59.928585 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/checkpoint-2067-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:35:00.164025 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/checkpoint-2067-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6489a8494cd847dd8171c6034a23f168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.564000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:37:21.105225 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/checkpoint-2756-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:37:21.341776 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/checkpoint-2756-epoch-4/pytorch_model.bin\n",
      "I0625 17:37:21.927116 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:37:22.163389 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/pytorch_model.bin\n",
      "I0625 17:37:22.176560 140553350014784 language_modeling_model.py:402]  Training of bert model complete. Saved to round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/.\n",
      "I0625 17:37:22.177371 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/config.json\n",
      "I0625 17:37:22.178039 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 17:37:22.178544 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/pytorch_model.bin\n",
      "I0625 17:37:24.063698 140553350014784 modeling_utils.py:741] Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0625 17:37:24.064234 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "I0625 17:37:24.065099 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 17:37:24.065544 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/added_tokens.json. We won't load it.\n",
      "I0625 17:37:24.065903 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/vocab.txt\n",
      "I0625 17:37:24.066171 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 17:37:24.066417 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/special_tokens_map.json\n",
      "I0625 17:37:24.066663 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training2/tokenizer_config.json\n",
      "I0625 17:37:24.199586 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a862511c855e42928b5bf52f181bc33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4ccfda65d9422bb643edfc35b8434c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:37:33.795858 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9eddf2a7f24dbfa8d35a677346a6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.139768"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:42:58.364892 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-2000/config.json\n",
      "I0625 17:42:58.607948 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-2000/pytorch_model.bin\n",
      "I0625 17:42:59.083568 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:42:59.312790 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297f997cf9de4fb4b6cd1da7ff416384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.128257"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:48:24.093721 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-4000/config.json\n",
      "I0625 17:48:24.325356 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-4000/pytorch_model.bin\n",
      "I0625 17:48:24.798701 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:48:25.027295 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01866dcb6a64f56ab7f107a1a3cd5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.107811"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:53:49.931850 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-6000/config.json\n",
      "I0625 17:53:50.160874 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-6000/pytorch_model.bin\n",
      "I0625 17:53:50.637606 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:53:50.868974 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8be5719404c4f3e9176e0e1b818003d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.087089"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:59:15.616432 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-8000/config.json\n",
      "I0625 17:59:15.847768 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-8000/pytorch_model.bin\n",
      "I0625 17:59:16.349464 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:59:16.580554 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 17:59:17.053930 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:59:17.285992 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/pytorch_model.bin\n",
      "I0625 17:59:17.299704 140553350014784 classification_model.py:279]  Training of bert model complete. Saved to round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/.\n",
      "I0625 17:59:18.333104 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0625 17:59:19.343939 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0625 17:59:19.347179 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 17:59:19.659754 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at cache/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0625 17:59:21.772017 140553350014784 modeling_utils.py:741] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0625 17:59:21.772587 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "I0625 17:59:21.827611 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce477cb1655d41efa1e39d297d92b5a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189e7ae6f1df47a1b81e155a2d0fe317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11016.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 17:59:25.266865 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/bert_cached_lm_111_training.txt\n",
      "I0625 17:59:25.294078 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0877d5a6f24b18b008051d0cc6a0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d46a07192fe4ef89587a57c84db812c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.727689"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:01:45.800780 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/checkpoint-689-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:01:46.037191 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/checkpoint-689-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297578d0fa5d4495ab9eaf21bccdd0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 3.163157"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:04:07.099411 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/checkpoint-1378-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:04:07.332697 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/checkpoint-1378-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc800786eee4d919d89a02a44cc83b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.040318"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:06:14.921656 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.299685"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:06:15.154520 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.496606"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:06:29.219483 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/checkpoint-2067-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:06:29.452980 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/checkpoint-2067-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26603373bb5b44c196a64d7c9d87f0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.005138"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:08:50.540066 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/checkpoint-2756-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:08:50.773401 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/checkpoint-2756-epoch-4/pytorch_model.bin\n",
      "I0625 18:08:51.245803 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:08:51.479889 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/pytorch_model.bin\n",
      "I0625 18:08:51.492758 140553350014784 language_modeling_model.py:402]  Training of bert model complete. Saved to round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/.\n",
      "I0625 18:08:51.493539 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/config.json\n",
      "I0625 18:08:51.494104 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 18:08:51.494559 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/pytorch_model.bin\n",
      "I0625 18:08:53.382463 140553350014784 modeling_utils.py:741] Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0625 18:08:53.383007 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "I0625 18:08:53.383794 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 18:08:53.384245 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/added_tokens.json. We won't load it.\n",
      "I0625 18:08:53.384638 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/vocab.txt\n",
      "I0625 18:08:53.384914 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 18:08:53.385163 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/special_tokens_map.json\n",
      "I0625 18:08:53.385414 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training3/tokenizer_config.json\n",
      "I0625 18:08:53.634049 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e19e2b4126f44f349837356d3bb08d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49909bd348f9419facf1e4fc353baabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:09:03.178256 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d60cf6d8d8452aaacdcd4d3fe8b0d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.152098"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:14:27.858150 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-2000/config.json\n",
      "I0625 18:14:28.075883 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-2000/pytorch_model.bin\n",
      "I0625 18:14:28.498512 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:14:28.702595 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e0b763c7bba4f659b942046149cc7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.131639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:19:53.705824 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-4000/config.json\n",
      "I0625 18:19:53.908575 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-4000/pytorch_model.bin\n",
      "I0625 18:19:54.329536 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:19:54.532421 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f71f9d6c18840f5a8dc565723bc6eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.097863"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:25:19.486749 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-6000/config.json\n",
      "I0625 18:25:19.690397 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-6000/pytorch_model.bin\n",
      "I0625 18:25:20.114650 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:25:20.321292 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6101baaf20144caa8b8f9356ee58d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.124774"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:30:45.712957 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-8000/config.json\n",
      "I0625 18:30:45.916263 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-8000/pytorch_model.bin\n",
      "I0625 18:30:46.340424 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:30:46.545147 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 18:30:46.968733 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:30:47.173743 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/pytorch_model.bin\n",
      "I0625 18:30:47.186679 140553350014784 classification_model.py:279]  Training of bert model complete. Saved to round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/.\n",
      "I0625 18:30:48.150057 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0625 18:30:49.107764 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0625 18:30:49.108741 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 18:30:49.426413 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at cache/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0625 18:30:51.558898 140553350014784 modeling_utils.py:741] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0625 18:30:51.559504 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "I0625 18:30:51.612158 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7e9da98b734618ae72423866f1e91b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61309b182d704f68abc217859ab82c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11016.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:30:55.061752 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/bert_cached_lm_111_training.txt\n",
      "I0625 18:30:55.091846 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5853b39f567b43e6943a3df7b3731975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efcd3c2fe86419cb259038298a18fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.602590"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:33:15.778424 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/checkpoint-689-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:33:16.005241 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/checkpoint-689-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201ad17230dc41058c2267562ad36536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 3.077931"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:35:37.138008 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/checkpoint-1378-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:35:37.351001 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/checkpoint-1378-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efda5a8245fd459b8599bff56fc8430e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.360803"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:37:44.793465 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 1.802327"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:37:45.007257 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.715663"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:37:59.031596 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/checkpoint-2067-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:37:59.242835 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/checkpoint-2067-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a61827bbc6a491897a396ae153497df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.831950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:40:20.335670 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/checkpoint-2756-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:40:20.549353 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/checkpoint-2756-epoch-4/pytorch_model.bin\n",
      "I0625 18:40:20.980758 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:40:21.195404 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/pytorch_model.bin\n",
      "I0625 18:40:21.208920 140553350014784 language_modeling_model.py:402]  Training of bert model complete. Saved to round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/.\n",
      "I0625 18:40:21.209707 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/config.json\n",
      "I0625 18:40:21.210326 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 18:40:21.210948 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/pytorch_model.bin\n",
      "I0625 18:40:23.065565 140553350014784 modeling_utils.py:741] Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0625 18:40:23.066101 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "I0625 18:40:23.066899 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 18:40:23.067339 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/added_tokens.json. We won't load it.\n",
      "I0625 18:40:23.067696 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/vocab.txt\n",
      "I0625 18:40:23.067954 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 18:40:23.068262 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/special_tokens_map.json\n",
      "I0625 18:40:23.068503 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training4/tokenizer_config.json\n",
      "I0625 18:40:23.314777 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2a13dd091743f3afb607bd785d3d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef38cefe49fa49cca560728bb17e4161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:40:32.677383 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a704725e34f42a3a24f2a43b8040aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.106829"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:45:57.737631 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-2000/config.json\n",
      "I0625 18:45:57.952622 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-2000/pytorch_model.bin\n",
      "I0625 18:45:58.378206 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:45:58.582007 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8328af6122254bdd9910c06dff2af51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.129226"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:51:23.699928 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-4000/config.json\n",
      "I0625 18:51:23.904244 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-4000/pytorch_model.bin\n",
      "I0625 18:51:24.329437 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:51:24.534140 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6a334f685f470c93e6cf589a343e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.130802"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:56:49.526349 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-6000/config.json\n",
      "I0625 18:56:49.731282 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-6000/pytorch_model.bin\n",
      "I0625 18:56:50.160838 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 18:56:50.366624 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f900dd316da44d739226da71a4d2b985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.077243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:02:15.447304 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-8000/config.json\n",
      "I0625 19:02:15.656471 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-8000/pytorch_model.bin\n",
      "I0625 19:02:16.095261 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:02:16.303761 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 19:02:16.738544 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:02:16.948457 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/pytorch_model.bin\n",
      "I0625 19:02:16.960951 140553350014784 classification_model.py:279]  Training of bert model complete. Saved to round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/.\n",
      "I0625 19:02:17.898668 140553350014784 tokenization_utils.py:1015] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at cache/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "I0625 19:02:18.898559 140553350014784 configuration_utils.py:285] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at cache/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "I0625 19:02:18.901098 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 19:02:19.215285 140553350014784 modeling_utils.py:650] loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at cache/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "I0625 19:02:21.270918 140553350014784 modeling_utils.py:741] Weights of BertForMaskedLM not initialized from pretrained model: ['cls.predictions.decoder.bias']\n",
      "I0625 19:02:21.271477 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "I0625 19:02:21.324680 140553350014784 language_modeling_utils.py:181]  Creating features from dataset file at cache/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8db59581a734a5e839b5d6756ed5d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=48944.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710a169418e74f42a9d945b5122f528a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11016.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:02:24.732591 140553350014784 language_modeling_utils.py:229]  Saving features into cached file cache/bert_cached_lm_111_training.txt\n",
      "I0625 19:02:24.759791 140553350014784 language_modeling_model.py:473]  Training started\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a43cc325ad4bbc8cab7f1138205831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38a445af7d146d5abebf32fd37c7d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.461731"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:04:45.519826 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/checkpoint-689-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:04:45.747215 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/checkpoint-689-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50dfa709ce2a4347bf8d7961e6ea0909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.547904"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:07:06.931057 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/checkpoint-1378-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:07:07.147374 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/checkpoint-1378-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdea6a4522ea4e229569c1ccedec4c6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 3.028809"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:09:14.789283 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/checkpoint-2000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Running loss: 2.132807"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:09:15.004537 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/checkpoint-2000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.665927"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:09:29.060270 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/checkpoint-2067-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:09:29.275296 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/checkpoint-2067-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1cacbfa3b24834a869632a7f0b54bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=689.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.103053"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:11:50.574160 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/checkpoint-2756-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:11:50.787489 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/checkpoint-2756-epoch-4/pytorch_model.bin\n",
      "I0625 19:11:51.212299 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:11:51.426337 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/pytorch_model.bin\n",
      "I0625 19:11:51.440664 140553350014784 language_modeling_model.py:402]  Training of bert model complete. Saved to round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/.\n",
      "I0625 19:11:51.441552 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/config.json\n",
      "I0625 19:11:51.442153 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 19:11:51.442660 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/pytorch_model.bin\n",
      "I0625 19:11:53.299659 140553350014784 modeling_utils.py:741] Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0625 19:11:53.300173 140553350014784 modeling_utils.py:747] Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias']\n",
      "I0625 19:11:53.301060 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 19:11:53.301605 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/added_tokens.json. We won't load it.\n",
      "I0625 19:11:53.302006 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/vocab.txt\n",
      "I0625 19:11:53.302418 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 19:11:53.302751 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/special_tokens_map.json\n",
      "I0625 19:11:53.303129 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_LM/outputs_bert_uncased_LM_training5/tokenizer_config.json\n",
      "I0625 19:11:53.519720 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0803d78fb84d4662bd9b978ba1cc724a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7af4d04f214f8e8889c2f01c10cf5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=4.0, style=ProgressStyle(description_width='iâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:12:03.014657 140553350014784 classification_model.py:367]    Starting fine-tuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588de075caa34caa814382946d3a7222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.136840"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:17:28.429250 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-2000/config.json\n",
      "I0625 19:17:28.643709 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-2000/pytorch_model.bin\n",
      "I0625 19:17:29.078743 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-2000-epoch-1/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:17:29.292760 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-2000-epoch-1/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5335a1822a9f4808b87f909ac292023e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.110204"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:22:54.751304 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-4000/config.json\n",
      "I0625 19:22:54.960883 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-4000/pytorch_model.bin\n",
      "I0625 19:22:55.387359 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-4000-epoch-2/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:22:55.598147 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-4000-epoch-2/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adbe73ff95a42cfa94f860d53f9016e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.110675"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:28:21.044556 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-6000/config.json\n",
      "I0625 19:28:21.254381 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-6000/pytorch_model.bin\n",
      "I0625 19:28:21.681217 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-6000-epoch-3/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:28:21.891513 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-6000-epoch-3/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00afcb6c645943e1855ba565fbfaa687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=2000.0, style=ProgressStyle(descrâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.127240"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:33:47.782159 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-8000/config.json\n",
      "I0625 19:33:47.992037 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-8000/pytorch_model.bin\n",
      "I0625 19:33:48.419595 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-8000-epoch-4/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:33:48.629297 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/checkpoint-8000-epoch-4/pytorch_model.bin\n",
      "I0625 19:33:49.057519 140553350014784 configuration_utils.py:144] Configuration saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 19:33:49.267709 140553350014784 modeling_utils.py:483] Model weights saved in round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/pytorch_model.bin\n",
      "I0625 19:33:49.281606 140553350014784 classification_model.py:279]  Training of bert model complete. Saved to round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/.\n"
     ]
    }
   ],
   "source": [
    "df_forLM = df_new.copy()\n",
    "df_forLM['text'] = ['[CLS] '] + df_new['text_a'] + [' [SEP] '] + df_new['text_b'] + [' [SEP]']\n",
    "df_forLM['text'].to_csv(r'LM/training.txt', header=None, index=None, sep=' ')\n",
    "for i in range(1, 6):\n",
    "    train_lm_args = {\n",
    "        \"output_dir\": \"round1_blend_model/round1_LM/outputs_bert_uncased_LM_training{}/\".format(i),\n",
    "        \"cache_dir\": \"cache/\",\n",
    "        \"best_model_dir\": \"LM/outputs/best_model/\",\n",
    "\n",
    "        \"fp16\": False,\n",
    "        \"fp16_opt_level\": \"O1\",\n",
    "        \"max_seq_length\": 113,\n",
    "        \"train_batch_size\": 16,\n",
    "        \"eval_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 4e-5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"do_lower_case\": False,\n",
    "    }\n",
    "    model = LanguageModelingModel(\"bert\", \"bert-base-uncased\", args=train_lm_args)\n",
    "    model.train_model('LM/training.txt')\n",
    "    train_args = {\n",
    "        \"output_dir\": \"round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training{}/\".format(i),\n",
    "        \"cache_dir\": \"cache/\",\n",
    "        \"best_model_dir\": \"model_results_gpu/outputs/best_model/\",\n",
    "\n",
    "        \"fp16\": False,\n",
    "        \"fp16_opt_level\": \"O1\",\n",
    "        \"max_seq_length\": 113,\n",
    "        \"train_batch_size\": 16,\n",
    "        \"eval_batch_size\": 16,\n",
    "        \"gradient_accumulation_steps\": 1,\n",
    "        \"num_train_epochs\": 4,\n",
    "        \"weight_decay\": 0,\n",
    "        \"learning_rate\": 4e-5,\n",
    "        \"adam_epsilon\": 1e-8,\n",
    "        \"warmup_ratio\": 0.06,\n",
    "        \"warmup_steps\": 0,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"do_lower_case\": False,\n",
    "\n",
    "        \"logging_steps\": 50,\n",
    "        \"evaluate_during_training\": False,\n",
    "        \"evaluate_during_training_steps\": 2000,\n",
    "        \"evaluate_during_training_verbose\": False,\n",
    "        \"use_cached_eval_features\": False,\n",
    "        \"save_eval_checkpoints\": False,\n",
    "        \"save_steps\": 2000,\n",
    "        \"no_cache\": True,\n",
    "        \"save_model_every_epoch\": True,\n",
    "        \"tensorboard_dir\": None,\n",
    "\n",
    "        \"overwrite_output_dir\": False,\n",
    "        \"reprocess_input_data\": True,\n",
    "\n",
    "        \"n_gpu\": 1,\n",
    "        \"silent\": False,\n",
    "        \"use_multiprocessing\": False,\n",
    "\n",
    "        \"wandb_project\": None,\n",
    "        \"wandb_kwargs\": {},\n",
    "\n",
    "        \"use_early_stopping\": True,\n",
    "        \"early_stopping_patience\": 3,\n",
    "        \"early_stopping_delta\": 0,\n",
    "        \"early_stopping_metric\": \"eval_loss\",\n",
    "        \"early_stopping_metric_minimize\": True,\n",
    "\n",
    "        \"manual_seed\": None,\n",
    "        \"encoding\": None,\n",
    "        \"config\": {}\n",
    "    }\n",
    "    model = MultiLabelClassificationModel('bert', 'round1_blend_model/round1_LM/outputs_bert_uncased_LM_training{}/'.format(i), num_labels=43, args=train_args)\n",
    "    model.train_model(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict results [pair sentance](https://simpletransformers.ai/docs/classification-data-formats/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = []\n",
    "for i in range(len(df_test_result)):\n",
    "    text_reply = '<s> ' + df_test_result['text_a'][i] + ' </s></s> ' + df_test_result['text_b'][i] + ' <s>'\n",
    "    to_predict.append(text_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:24:17.790860 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/config.json\n",
      "I0625 20:24:17.794344 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:24:17.796649 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/pytorch_model.bin\n",
      "I0625 20:24:24.175107 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training1/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training1/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:24:24.176086 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/added_tokens.json. We won't load it.\n",
      "I0625 20:24:24.176519 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/vocab.json\n",
      "I0625 20:24:24.176759 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/merges.txt\n",
      "I0625 20:24:24.177009 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:24:24.177331 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/special_tokens_map.json\n",
      "I0625 20:24:24.177588 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/tokenizer_config.json\n",
      "I0625 20:24:24.382927 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5a4c84e7a645acb67c59393a759dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e12ad9870d4f39ab3a46cd5e372a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:24:38.277387 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/config.json\n",
      "I0625 20:24:38.278079 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:24:38.278570 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/pytorch_model.bin\n",
      "I0625 20:24:44.669268 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training2/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training2/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:24:44.670127 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/added_tokens.json. We won't load it.\n",
      "I0625 20:24:44.670556 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/vocab.json\n",
      "I0625 20:24:44.670825 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/merges.txt\n",
      "I0625 20:24:44.671083 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:24:44.671386 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/special_tokens_map.json\n",
      "I0625 20:24:44.671630 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/tokenizer_config.json\n",
      "I0625 20:24:44.819972 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c527e1b26800408faf5520b2be762a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0db79ec056b4868825a037ee02a12bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:24:58.606472 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/config.json\n",
      "I0625 20:24:58.607131 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:24:58.607545 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/pytorch_model.bin\n",
      "I0625 20:25:04.951267 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training3/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training3/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:25:04.951935 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/added_tokens.json. We won't load it.\n",
      "I0625 20:25:04.952400 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/vocab.json\n",
      "I0625 20:25:04.952691 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/merges.txt\n",
      "I0625 20:25:04.952943 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:25:04.953193 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/special_tokens_map.json\n",
      "I0625 20:25:04.953498 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/tokenizer_config.json\n",
      "I0625 20:25:05.197773 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a339a65398174f2e9dcde32f87c0b11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "261345c8d43e4b269e2e576c4c13d30a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:25:18.991020 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/config.json\n",
      "I0625 20:25:18.991648 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:25:18.992051 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/pytorch_model.bin\n",
      "I0625 20:25:26.001213 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training4/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training4/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:25:26.001919 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/added_tokens.json. We won't load it.\n",
      "I0625 20:25:26.002378 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/vocab.json\n",
      "I0625 20:25:26.002641 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/merges.txt\n",
      "I0625 20:25:26.002911 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:25:26.003168 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/special_tokens_map.json\n",
      "I0625 20:25:26.003482 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/tokenizer_config.json\n",
      "I0625 20:25:26.165007 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6a9227f0454a1ca14f437846fe4a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef23431ed67741b88ffba3be0d89fff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:25:40.014109 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/config.json\n",
      "I0625 20:25:40.014735 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:25:40.015198 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/pytorch_model.bin\n",
      "I0625 20:25:47.021797 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training5/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training5/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:25:47.022437 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/added_tokens.json. We won't load it.\n",
      "I0625 20:25:47.022963 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/vocab.json\n",
      "I0625 20:25:47.023258 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/merges.txt\n",
      "I0625 20:25:47.023516 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:25:47.023765 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/special_tokens_map.json\n",
      "I0625 20:25:47.024077 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/tokenizer_config.json\n",
      "I0625 20:25:47.311175 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6242216c492043e9ae9dd9569680c172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a88ca626c824ac0b19a8350e1b89b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:26:01.160101 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/config.json\n",
      "I0625 20:26:01.160797 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:26:01.161309 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/pytorch_model.bin\n",
      "I0625 20:26:06.170522 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training6/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training6/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:26:06.171379 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/added_tokens.json. We won't load it.\n",
      "I0625 20:26:06.171815 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/vocab.json\n",
      "I0625 20:26:06.172077 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/merges.txt\n",
      "I0625 20:26:06.172335 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:26:06.172647 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/special_tokens_map.json\n",
      "I0625 20:26:06.172893 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/tokenizer_config.json\n",
      "I0625 20:26:06.379803 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243545d8fce94a06a76e61474a35d484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a04d0dd7f7e46fa9515d71a7d492426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:26:20.254206 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/config.json\n",
      "I0625 20:26:20.254834 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:26:20.255264 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/pytorch_model.bin\n",
      "I0625 20:26:25.324847 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training7/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training7/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:26:25.325499 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/added_tokens.json. We won't load it.\n",
      "I0625 20:26:25.325949 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/vocab.json\n",
      "I0625 20:26:25.326203 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/merges.txt\n",
      "I0625 20:26:25.326456 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:26:25.326709 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/special_tokens_map.json\n",
      "I0625 20:26:25.327024 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/tokenizer_config.json\n",
      "I0625 20:26:25.526287 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082445fa526c4eac95105c03215e5842",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d2653dab1b24799959ce9e2aa05dc22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:26:39.487960 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/config.json\n",
      "I0625 20:26:39.488617 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:26:39.489196 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/pytorch_model.bin\n",
      "I0625 20:26:44.599485 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training8/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training8/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:26:44.600438 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/added_tokens.json. We won't load it.\n",
      "I0625 20:26:44.600823 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/vocab.json\n",
      "I0625 20:26:44.601089 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/merges.txt\n",
      "I0625 20:26:44.601342 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:26:44.601599 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/special_tokens_map.json\n",
      "I0625 20:26:44.601845 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/tokenizer_config.json\n",
      "I0625 20:26:44.798451 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7679d4f08124e978a8f8f117f25a1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04d0b63da3a7499b89a2a03e14265ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:26:58.689223 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/config.json\n",
      "I0625 20:26:58.689846 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:26:58.690291 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/pytorch_model.bin\n",
      "I0625 20:27:03.884476 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training9/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training9/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:27:03.885129 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/added_tokens.json. We won't load it.\n",
      "I0625 20:27:03.885942 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/vocab.json\n",
      "I0625 20:27:03.886230 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/merges.txt\n",
      "I0625 20:27:03.886533 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:27:03.886844 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/special_tokens_map.json\n",
      "I0625 20:27:03.887209 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/tokenizer_config.json\n",
      "I0625 20:27:04.088954 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1699fb3db2e401d9026b3f93c831064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bfcb6250e743b5803a6b8b731d33f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:27:17.926090 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/config.json\n",
      "I0625 20:27:17.926837 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:27:17.927366 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/pytorch_model.bin\n",
      "I0625 20:27:23.157024 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training10/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training10/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:27:23.157684 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/added_tokens.json. We won't load it.\n",
      "I0625 20:27:23.158107 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/vocab.json\n",
      "I0625 20:27:23.158379 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/merges.txt\n",
      "I0625 20:27:23.158630 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:27:23.158896 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/special_tokens_map.json\n",
      "I0625 20:27:23.159175 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/tokenizer_config.json\n",
      "I0625 20:27:23.364900 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6baa7e54859e4b1982f3c7eb6616ff22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfd80aa17ae41cd8be461a48abddb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model_idx in range(1, 11):\n",
    "    model_roberta_base = MultiLabelClassificationModel('roberta', 'round1_blend_model/round1_MLR/outputs_roberta_LM_training' + str(model_idx) + '/')\n",
    "    predictions_roberta_base, raw_outputs_roberta_base = model_roberta_base.predict(to_predict)\n",
    "    with open(\"./final_results/test/roberta_\" + str(model_idx) + \".csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(raw_outputs_roberta_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = []\n",
    "for i in range(len(df_test_result)):\n",
    "    text_reply = '[CLS] ' + df_test_result['text_a'][i] + ' [SEP] ' + df_test_result['text_b'][i] + ' [SEP]'\n",
    "    to_predict.append(text_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:28:40.091176 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/config.json\n",
      "I0625 20:28:40.092066 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 20:28:40.092571 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/pytorch_model.bin\n",
      "I0625 20:28:44.789581 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:28:44.790378 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/added_tokens.json. We won't load it.\n",
      "I0625 20:28:44.790782 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/vocab.txt\n",
      "I0625 20:28:44.791064 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:28:44.791317 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/special_tokens_map.json\n",
      "I0625 20:28:44.791569 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/tokenizer_config.json\n",
      "I0625 20:28:44.915192 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded45fa132b443a999b7d636fc07c43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0859a669d1d54b41b88f13cc44a176d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:28:58.894474 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/config.json\n",
      "I0625 20:28:58.895091 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 20:28:58.895531 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/pytorch_model.bin\n",
      "I0625 20:29:03.525206 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:29:03.525836 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/added_tokens.json. We won't load it.\n",
      "I0625 20:29:03.526314 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/vocab.txt\n",
      "I0625 20:29:03.526637 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:29:03.526996 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/special_tokens_map.json\n",
      "I0625 20:29:03.527347 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/tokenizer_config.json\n",
      "I0625 20:29:03.625520 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39092a23780f407e93e337da64054b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a1ff7d35e7b4dc38cc6384a6db71f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:29:17.669618 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/config.json\n",
      "I0625 20:29:17.670567 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 20:29:17.670971 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/pytorch_model.bin\n",
      "I0625 20:29:22.409741 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:29:22.410551 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/added_tokens.json. We won't load it.\n",
      "I0625 20:29:22.410942 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/vocab.txt\n",
      "I0625 20:29:22.411216 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:29:22.411471 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/special_tokens_map.json\n",
      "I0625 20:29:22.411978 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/tokenizer_config.json\n",
      "I0625 20:29:22.621744 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ad8163dcdf42dfa0df173f43a037fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96534a3abcb42e48d6bb24374abaaa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:29:36.610948 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/config.json\n",
      "I0625 20:29:36.611679 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 20:29:36.612096 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/pytorch_model.bin\n",
      "I0625 20:29:38.383473 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:29:38.384087 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/added_tokens.json. We won't load it.\n",
      "I0625 20:29:38.384490 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/vocab.txt\n",
      "I0625 20:29:38.384775 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:29:38.385031 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/special_tokens_map.json\n",
      "I0625 20:29:38.385288 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/tokenizer_config.json\n",
      "I0625 20:29:38.463847 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa01af240bf4220b857c5ea7a818b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71dcadebfe394e5a99a5658efc862b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:29:52.495178 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/config.json\n",
      "I0625 20:29:52.495778 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 20:29:52.496161 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/pytorch_model.bin\n",
      "I0625 20:29:54.260776 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:29:54.261390 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/added_tokens.json. We won't load it.\n",
      "I0625 20:29:54.261832 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/vocab.txt\n",
      "I0625 20:29:54.262134 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:29:54.262418 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/special_tokens_map.json\n",
      "I0625 20:29:54.262730 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/tokenizer_config.json\n",
      "I0625 20:29:54.342454 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8412cc098848bc800178e419911780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786a71c6e95a44dd867361eb3bdfc8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:30:08.414053 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/config.json\n",
      "I0625 20:30:08.414676 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 20:30:08.415166 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/pytorch_model.bin\n",
      "I0625 20:30:12.828593 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:30:12.829288 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/added_tokens.json. We won't load it.\n",
      "I0625 20:30:12.829713 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/vocab.txt\n",
      "I0625 20:30:12.830003 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:30:12.830278 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/special_tokens_map.json\n",
      "I0625 20:30:12.830544 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/tokenizer_config.json\n",
      "I0625 20:30:12.933639 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a121a4b6b14db99a60a22326038e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3abb3072d3462e8a7ba3a2ce3269ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:30:27.004646 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/config.json\n",
      "I0625 20:30:27.005337 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 20:30:27.005850 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/pytorch_model.bin\n",
      "I0625 20:30:31.341072 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:30:31.342088 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/added_tokens.json. We won't load it.\n",
      "I0625 20:30:31.342474 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/vocab.txt\n",
      "I0625 20:30:31.342737 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:30:31.342994 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/special_tokens_map.json\n",
      "I0625 20:30:31.343252 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/tokenizer_config.json\n",
      "I0625 20:30:31.443112 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4601d04b977c4f5b9b62dd6bcad010f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ea4fb666524afaa2f1354b96b0db5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:30:45.539209 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/config.json\n",
      "I0625 20:30:45.539832 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 20:30:45.540300 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/pytorch_model.bin\n",
      "I0625 20:30:49.909875 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:30:49.910509 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/added_tokens.json. We won't load it.\n",
      "I0625 20:30:49.911043 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/vocab.txt\n",
      "I0625 20:30:49.911341 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:30:49.911592 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/special_tokens_map.json\n",
      "I0625 20:30:49.911843 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/tokenizer_config.json\n",
      "I0625 20:30:50.017740 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d320fd4a1675495ca8b122928ad5d7d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f84fa98c43e3405fa981cf74db1f0abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:31:04.230490 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/config.json\n",
      "I0625 20:31:04.231113 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 20:31:04.231558 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/pytorch_model.bin\n",
      "I0625 20:31:08.591143 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:31:08.591853 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/added_tokens.json. We won't load it.\n",
      "I0625 20:31:08.592255 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/vocab.txt\n",
      "I0625 20:31:08.592524 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:31:08.592779 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/special_tokens_map.json\n",
      "I0625 20:31:08.593022 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/tokenizer_config.json\n",
      "I0625 20:31:08.686061 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc40566770424cd2a2aaff8aa541f806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544674901c7c4323912e4e536b060896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:31:22.781734 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/config.json\n",
      "I0625 20:31:22.782361 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 20:31:22.782787 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/pytorch_model.bin\n",
      "I0625 20:31:27.227623 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:31:27.228467 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/added_tokens.json. We won't load it.\n",
      "I0625 20:31:27.228929 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/vocab.txt\n",
      "I0625 20:31:27.229172 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:31:27.229426 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/special_tokens_map.json\n",
      "I0625 20:31:27.229734 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/tokenizer_config.json\n",
      "I0625 20:31:27.332177 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7dd215e37234f0888fa9229ca42f5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a1b755a0c447578d4e8c678d944add",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for bert_type in ['uncased', 'cased']:\n",
    "    for model_idx in range(1, 6):\n",
    "        model_bert_base = MultiLabelClassificationModel('bert', 'round1_blend_model/round1_MLR/outputs_bert_' + bert_type + '_LM_training' + str(model_idx) + '/')\n",
    "        predictions_bert_base, raw_outputs_bert_base = model_bert_base.predict(to_predict)\n",
    "        with open(\"./final_results/test/bert_\" + bert_type + \"_\" + str(model_idx) + \".csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(raw_outputs_bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = listdir('./final_results/test/')\n",
    "roberta_prob = np.zeros([len(df_test_result), len(categories_mapping)])\n",
    "bert_uncase_prob = np.zeros([len(df_test_result), len(categories_mapping)])\n",
    "bert_case_prob = np.zeros([len(df_test_result), len(categories_mapping)])\n",
    "for f in files:\n",
    "    if f.find('.') != 0:\n",
    "        model_prob = pd.read_csv('./final_results/test/' + f, header=None)\n",
    "        probability_list = model_prob.values.tolist()\n",
    "        if f.find('roberta') >= 0:\n",
    "            roberta_prob = np.add(roberta_prob, probability_list)\n",
    "        if f.find('_cased_') >= 0:\n",
    "            bert_case_prob = np.add(bert_case_prob, probability_list)\n",
    "        if f.find('uncased') >= 0:\n",
    "            bert_uncase_prob = np.add(bert_uncase_prob, probability_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_count = 10\n",
    "bert_count = 5\n",
    "roberta_prob /= roberta_count\n",
    "bert_case_prob /= bert_count\n",
    "bert_uncase_prob /= bert_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 3.5\n",
    "total_predict = []\n",
    "for eid, row_roberta_base in enumerate(roberta_prob):\n",
    "    row = (row_roberta_base ** P) * 3.0 + \\\n",
    "            (bert_case_prob[eid] ** P) * 0.8 + (bert_uncase_prob[eid] ** P) * 0.8 \n",
    "    predict_class = []\n",
    "    correct = 0\n",
    "    sort_index = sorted(range(len(row)), key=lambda k: row[k], reverse=True)\n",
    "    for key, value in categories_mapping.items():\n",
    "        if value in sort_index[0:6]:\n",
    "            predict_class.append(key)\n",
    "            \n",
    "    total_predict.append(predict_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "      <th>reply</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36000</td>\n",
       "      <td>@Youngdeji_ I think if uzi and carti dropping ...</td>\n",
       "      <td></td>\n",
       "      <td>[agree, dance, happy_dance, no, please, yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36001</td>\n",
       "      <td>For the third year in a row weâ€™re discussing t...</td>\n",
       "      <td></td>\n",
       "      <td>[eye_roll, facepalm, no, seriously, sigh, smh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36002</td>\n",
       "      <td>dababy album sounds like it was made for nigga...</td>\n",
       "      <td>That's why you bought it.</td>\n",
       "      <td>[agree, omg, seriously, shocked, smh, yes]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36003</td>\n",
       "      <td>Majority of Indians donâ€™t watch any sport othe...</td>\n",
       "      <td>@ZairaWasimmm got a great story because of the...</td>\n",
       "      <td>[agree, applause, oh_snap, shocked, slow_clap,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36004</td>\n",
       "      <td>everybody is just now listening to @madisonbee...</td>\n",
       "      <td></td>\n",
       "      <td>[facepalm, idk, seriously, shocked, sigh, smh]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx                                               text  \\\n",
       "0  36000  @Youngdeji_ I think if uzi and carti dropping ...   \n",
       "1  36001  For the third year in a row weâ€™re discussing t...   \n",
       "2  36002  dababy album sounds like it was made for nigga...   \n",
       "3  36003  Majority of Indians donâ€™t watch any sport othe...   \n",
       "4  36004  everybody is just now listening to @madisonbee...   \n",
       "\n",
       "                                               reply  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2                          That's why you bought it.   \n",
       "3  @ZairaWasimmm got a great story because of the...   \n",
       "4                                                      \n",
       "\n",
       "                                          categories  \n",
       "0       [agree, dance, happy_dance, no, please, yes]  \n",
       "1     [eye_roll, facepalm, no, seriously, sigh, smh]  \n",
       "2         [agree, omg, seriously, shocked, smh, yes]  \n",
       "3  [agree, applause, oh_snap, shocked, slow_clap,...  \n",
       "4     [facepalm, idk, seriously, shocked, sigh, smh]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_ori['categories'] = total_predict\n",
    "df_test_ori.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_ori.to_json('./results/eval.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = []\n",
    "for i in range(len(df_dev_result)):\n",
    "    text_reply = '<s> ' + df_dev_result['text_a'][i] + ' </s></s> ' + df_dev_result['text_b'][i] + ' <s>'\n",
    "    to_predict.append(text_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:34:01.536793 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/config.json\n",
      "I0625 20:34:01.537540 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:34:01.537962 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/pytorch_model.bin\n",
      "I0625 20:34:04.043116 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training1/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training1/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:34:04.043967 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/added_tokens.json. We won't load it.\n",
      "I0625 20:34:04.044398 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/vocab.json\n",
      "I0625 20:34:04.044664 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/merges.txt\n",
      "I0625 20:34:04.044919 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:34:04.045178 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/special_tokens_map.json\n",
      "I0625 20:34:04.045487 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training1/tokenizer_config.json\n",
      "I0625 20:34:04.195369 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b021cb97306458db8a50e1c90e13f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b4084408ac456dbf91624dad1c96a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:34:17.969518 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/config.json\n",
      "I0625 20:34:17.970340 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:34:17.970691 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:34:20.560892 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training2/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training2/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:34:20.561494 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/added_tokens.json. We won't load it.\n",
      "I0625 20:34:20.561860 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/vocab.json\n",
      "I0625 20:34:20.562178 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/merges.txt\n",
      "I0625 20:34:20.562509 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:34:20.562901 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/special_tokens_map.json\n",
      "I0625 20:34:20.563258 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training2/tokenizer_config.json\n",
      "I0625 20:34:20.672243 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bf869c32fb4ecfaaa12b4ff181303a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcc70bcfea76449aa4f8f8c6a66eb124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:34:34.621426 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/config.json\n",
      "I0625 20:34:34.622107 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:34:34.622510 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/pytorch_model.bin\n",
      "I0625 20:34:37.159631 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training3/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training3/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:34:37.160219 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/added_tokens.json. We won't load it.\n",
      "I0625 20:34:37.160622 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/vocab.json\n",
      "I0625 20:34:37.160899 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/merges.txt\n",
      "I0625 20:34:37.161145 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:34:37.161393 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/special_tokens_map.json\n",
      "I0625 20:34:37.161666 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training3/tokenizer_config.json\n",
      "I0625 20:34:37.269696 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6eed4f8a1943b29b113f25bbfad0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3088231e3d984b69866445334170646e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:34:51.088480 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/config.json\n",
      "I0625 20:34:51.089193 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:34:51.089653 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/pytorch_model.bin\n",
      "I0625 20:34:53.615206 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training4/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training4/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:34:53.615817 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/added_tokens.json. We won't load it.\n",
      "I0625 20:34:53.616242 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/vocab.json\n",
      "I0625 20:34:53.616533 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/merges.txt\n",
      "I0625 20:34:53.616790 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:34:53.617048 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/special_tokens_map.json\n",
      "I0625 20:34:53.617367 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training4/tokenizer_config.json\n",
      "I0625 20:34:53.722856 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed987ac3927d4b56962c34d10ace921b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2befbd79b9c346209fc745f5878bccc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:35:07.605582 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/config.json\n",
      "I0625 20:35:07.606216 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:35:07.606622 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:35:10.142784 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training5/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training5/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:35:10.143390 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/added_tokens.json. We won't load it.\n",
      "I0625 20:35:10.144034 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/vocab.json\n",
      "I0625 20:35:10.144589 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/merges.txt\n",
      "I0625 20:35:10.144896 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:35:10.145258 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/special_tokens_map.json\n",
      "I0625 20:35:10.145605 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training5/tokenizer_config.json\n",
      "I0625 20:35:10.257026 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc067cba6bc4d55a2ab8196f3401ddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb17562ebb44290b1354acd6b4676d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:35:24.128230 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/config.json\n",
      "I0625 20:35:24.128887 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:35:24.129291 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/pytorch_model.bin\n",
      "I0625 20:35:26.651571 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training6/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training6/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:35:26.652219 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/added_tokens.json. We won't load it.\n",
      "I0625 20:35:26.652671 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/vocab.json\n",
      "I0625 20:35:26.652941 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/merges.txt\n",
      "I0625 20:35:26.653187 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:35:26.653465 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/special_tokens_map.json\n",
      "I0625 20:35:26.653704 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training6/tokenizer_config.json\n",
      "I0625 20:35:26.886836 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675b4e8afa0d48ec8ef2621165aa11a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1faf93affed84340849a00a9366e8462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:35:40.752967 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/config.json\n",
      "I0625 20:35:40.753614 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:35:40.754049 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/pytorch_model.bin\n",
      "I0625 20:35:43.288540 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training7/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training7/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:35:43.289141 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/added_tokens.json. We won't load it.\n",
      "I0625 20:35:43.289573 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/vocab.json\n",
      "I0625 20:35:43.289822 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/merges.txt\n",
      "I0625 20:35:43.290072 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:35:43.290321 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/special_tokens_map.json\n",
      "I0625 20:35:43.290599 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training7/tokenizer_config.json\n",
      "I0625 20:35:43.398149 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfde7bfa39094a539b1ade121cc03656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770b9fec12ff47e69d492722be30a208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:35:57.272168 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/config.json\n",
      "I0625 20:35:57.272832 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:35:57.273293 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/pytorch_model.bin\n",
      "I0625 20:35:59.805210 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training8/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training8/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:35:59.805835 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/added_tokens.json. We won't load it.\n",
      "I0625 20:35:59.806366 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/vocab.json\n",
      "I0625 20:35:59.806696 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/merges.txt\n",
      "I0625 20:35:59.807002 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:35:59.807288 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/special_tokens_map.json\n",
      "I0625 20:35:59.807583 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training8/tokenizer_config.json\n",
      "I0625 20:35:59.913652 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d4eaf56ee04ae89c86efbb2611536f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5531a3fee9294fef8cb6eb9b18f9690f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:36:13.788202 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/config.json\n",
      "I0625 20:36:13.788918 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:36:13.789372 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/pytorch_model.bin\n",
      "I0625 20:36:16.327372 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training9/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training9/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:36:16.327969 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/added_tokens.json. We won't load it.\n",
      "I0625 20:36:16.328387 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/vocab.json\n",
      "I0625 20:36:16.328674 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/merges.txt\n",
      "I0625 20:36:16.328929 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:36:16.329182 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/special_tokens_map.json\n",
      "I0625 20:36:16.329493 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training9/tokenizer_config.json\n",
      "I0625 20:36:16.554173 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f3c13cedab40b7bfa858be339459bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d13939a0b149488a17ef2f955d3cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:36:30.441080 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:36:30.441716 140553350014784 configuration_utils.py:321] Model config RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "I0625 20:36:30.442275 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/pytorch_model.bin\n",
      "I0625 20:36:32.969703 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_roberta_LM_training10/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli, distilroberta-base, roberta-base-openai-detector, roberta-large-openai-detector). Assuming 'round1_blend_model/round1_MLR/outputs_roberta_LM_training10/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:36:32.970304 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/added_tokens.json. We won't load it.\n",
      "I0625 20:36:32.970701 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/vocab.json\n",
      "I0625 20:36:32.970984 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/merges.txt\n",
      "I0625 20:36:32.971241 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:36:32.971495 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/special_tokens_map.json\n",
      "I0625 20:36:32.971793 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_roberta_LM_training10/tokenizer_config.json\n",
      "I0625 20:36:33.078300 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7398256e92d4745b6374a5c0a09eefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d740892e90ed474f989eb5945b07dcc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for model_idx in range(1, 11):\n",
    "    model_roberta_base = MultiLabelClassificationModel('roberta', 'round1_blend_model/round1_MLR/outputs_roberta_LM_training' + str(model_idx) + '/')\n",
    "    predictions_roberta_base, raw_outputs_roberta_base = model_roberta_base.predict(to_predict)\n",
    "    with open(\"./final_results/dev/roberta_\" + str(model_idx) + \".csv\", \"w\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(raw_outputs_roberta_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict = []\n",
    "for i in range(len(df_dev_result)):\n",
    "    text_reply = '[CLS] ' + df_dev_result['text_a'][i] + ' [SEP] ' + df_dev_result['text_b'][i] + ' [SEP]'\n",
    "    to_predict.append(text_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:41:35.630744 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/config.json\n",
      "I0625 20:41:35.631663 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 20:41:35.632138 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/pytorch_model.bin\n",
      "I0625 20:41:37.409828 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:41:37.410433 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/added_tokens.json. We won't load it.\n",
      "I0625 20:41:37.410950 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/vocab.txt\n",
      "I0625 20:41:37.411214 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:41:37.411467 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/special_tokens_map.json\n",
      "I0625 20:41:37.411853 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training1/tokenizer_config.json\n",
      "I0625 20:41:37.531682 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b29bca435d6a4c2eb88cc72dd7ab4327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d605ec5b37422692bfacaa62290f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:41:51.503073 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/config.json\n",
      "I0625 20:41:51.503672 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 20:41:51.504243 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/pytorch_model.bin\n",
      "I0625 20:41:53.269730 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:41:53.270333 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/added_tokens.json. We won't load it.\n",
      "I0625 20:41:53.270888 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/vocab.txt\n",
      "I0625 20:41:53.271186 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:41:53.271445 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/special_tokens_map.json\n",
      "I0625 20:41:53.271697 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training2/tokenizer_config.json\n",
      "I0625 20:41:53.462026 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da178e20ee494308831ecbf82b9512c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2461670d221c41089fb5e0025f1e7128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:42:07.455550 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/config.json\n",
      "I0625 20:42:07.456245 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 20:42:07.456743 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/pytorch_model.bin\n",
      "I0625 20:42:09.228769 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:42:09.229426 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/added_tokens.json. We won't load it.\n",
      "I0625 20:42:09.229810 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/vocab.txt\n",
      "I0625 20:42:09.230096 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:42:09.230358 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/special_tokens_map.json\n",
      "I0625 20:42:09.230652 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training3/tokenizer_config.json\n",
      "I0625 20:42:09.308305 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32b72c877e3438e99461a1941b2fa05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8caa350eb2e047298258e3a5337cb266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:42:23.333124 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/config.json\n",
      "I0625 20:42:23.333827 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 20:42:23.334231 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/pytorch_model.bin\n",
      "I0625 20:42:25.108603 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:42:25.109292 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/added_tokens.json. We won't load it.\n",
      "I0625 20:42:25.109648 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/vocab.txt\n",
      "I0625 20:42:25.109974 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:42:25.110235 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/special_tokens_map.json\n",
      "I0625 20:42:25.110485 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training4/tokenizer_config.json\n",
      "I0625 20:42:25.190430 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900652d0440a4b67a1b5865deb2a33a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693693b494d24edabc6a4a94f1783723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:42:39.393156 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/config.json\n",
      "I0625 20:42:39.393832 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0625 20:42:39.394226 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/pytorch_model.bin\n",
      "I0625 20:42:41.160314 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:42:41.160906 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/added_tokens.json. We won't load it.\n",
      "I0625 20:42:41.161305 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/vocab.txt\n",
      "I0625 20:42:41.161582 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:42:41.161833 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/special_tokens_map.json\n",
      "I0625 20:42:41.162086 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_uncased_LM_training5/tokenizer_config.json\n",
      "I0625 20:42:41.241780 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cb7e96f4ae46a6a4ffcb0b0deef047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b44ada37c79486c8d4e9b3ffe42cb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:42:55.296054 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/config.json\n",
      "I0625 20:42:55.296774 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 20:42:55.297234 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/pytorch_model.bin\n",
      "I0625 20:42:57.041282 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:42:57.042073 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/added_tokens.json. We won't load it.\n",
      "I0625 20:42:57.042462 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/vocab.txt\n",
      "I0625 20:42:57.042735 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:42:57.043006 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/special_tokens_map.json\n",
      "I0625 20:42:57.043259 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training1/tokenizer_config.json\n",
      "I0625 20:42:57.118664 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cbf066a51ca419fa8c44f015ed14db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a99f81dc25b4b7ba7d4e74de5bfcfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:43:11.231738 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/config.json\n",
      "I0625 20:43:11.232401 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 20:43:11.232810 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/pytorch_model.bin\n",
      "I0625 20:43:12.989631 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:43:12.990240 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/added_tokens.json. We won't load it.\n",
      "I0625 20:43:12.990641 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/vocab.txt\n",
      "I0625 20:43:12.990933 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:43:12.991190 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/special_tokens_map.json\n",
      "I0625 20:43:12.991445 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training2/tokenizer_config.json\n",
      "I0625 20:43:13.068661 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5ece3fc9344ec6bf4f9911e441323c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5a7e282c7a4d5fbe3298dea032dfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:43:27.290642 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/config.json\n",
      "I0625 20:43:27.291420 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 20:43:27.291831 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/pytorch_model.bin\n",
      "I0625 20:43:29.042349 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:43:29.042982 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/added_tokens.json. We won't load it.\n",
      "I0625 20:43:29.043589 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/vocab.txt\n",
      "I0625 20:43:29.043908 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:43:29.044172 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/special_tokens_map.json\n",
      "I0625 20:43:29.044435 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training3/tokenizer_config.json\n",
      "I0625 20:43:29.121651 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e63d622b4d74df68941ab3b3977a0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e91230d7ce04babbe66e5856a3fe296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:43:43.246163 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/config.json\n",
      "I0625 20:43:43.246868 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 20:43:43.247318 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/pytorch_model.bin\n",
      "I0625 20:43:45.008732 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:43:45.009392 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/added_tokens.json. We won't load it.\n",
      "I0625 20:43:45.009911 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/vocab.txt\n",
      "I0625 20:43:45.010186 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:43:45.010429 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/special_tokens_map.json\n",
      "I0625 20:43:45.010665 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training4/tokenizer_config.json\n",
      "I0625 20:43:45.090801 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760bdbdd8a09458cadde902dc7d31db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d16f43404542e89361972ff1d34efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0625 20:43:59.186888 140553350014784 configuration_utils.py:283] loading configuration file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/config.json\n",
      "I0625 20:43:59.187621 140553350014784 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMultiLabelSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0625 20:43:59.188019 140553350014784 modeling_utils.py:648] loading weights file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/pytorch_model.bin\n",
      "I0625 20:44:00.939884 140553350014784 tokenization_utils.py:929] Model name 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0625 20:44:00.940480 140553350014784 tokenization_utils.py:958] Didn't find file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/added_tokens.json. We won't load it.\n",
      "I0625 20:44:00.941306 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/vocab.txt\n",
      "I0625 20:44:00.941601 140553350014784 tokenization_utils.py:1013] loading file None\n",
      "I0625 20:44:00.941913 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/special_tokens_map.json\n",
      "I0625 20:44:00.942214 140553350014784 tokenization_utils.py:1013] loading file round1_blend_model/round1_MLR/outputs_bert_cased_LM_training5/tokenizer_config.json\n",
      "I0625 20:44:01.018814 140553350014784 classification_model.py:801]  Converting to features started. Cache is not used.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc07f25eb95e447a9daef9d4d2416dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07ec3f8e2d1649dd9467490f9ed5c771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for bert_type in ['uncased', 'cased']:\n",
    "    for model_idx in range(1, 6):\n",
    "        model_bert_base = MultiLabelClassificationModel('bert', 'round1_blend_model/round1_MLR/outputs_bert_' + bert_type + '_LM_training' + str(model_idx) + '/')\n",
    "        predictions_bert_base, raw_outputs_bert_base = model_bert_base.predict(to_predict)\n",
    "        with open(\"./final_results/dev/bert_\" + bert_type + \"_\" + str(model_idx) + \".csv\", \"w\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(raw_outputs_bert_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = listdir('./final_results/dev/')\n",
    "roberta_prob = np.zeros([len(df_dev_result), len(categories_mapping)])\n",
    "bert_uncase_prob = np.zeros([len(df_dev_result), len(categories_mapping)])\n",
    "bert_case_prob = np.zeros([len(df_dev_result), len(categories_mapping)])\n",
    "for f in files:\n",
    "    if f.find('.') != 0:\n",
    "        model_prob = pd.read_csv('./final_results/dev/' + f, header=None)\n",
    "        probability_list = model_prob.values.tolist()\n",
    "        if f.find('roberta') >= 0:\n",
    "            roberta_prob = np.add(roberta_prob, probability_list)\n",
    "        if f.find('_cased_') >= 0:\n",
    "            bert_case_prob = np.add(bert_case_prob, probability_list)\n",
    "        if f.find('uncased') >= 0:\n",
    "            bert_uncase_prob = np.add(bert_uncase_prob, probability_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_count = 10\n",
    "bert_count = 5\n",
    "roberta_prob /= roberta_count\n",
    "bert_case_prob /= bert_count\n",
    "bert_uncase_prob /= bert_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 3.5\n",
    "total_predict = []\n",
    "for eid, row_roberta_base in enumerate(roberta_prob):\n",
    "    row = (row_roberta_base ** P) * 3.0 + \\\n",
    "            (bert_case_prob[eid] ** P) * 0.8 + (bert_uncase_prob[eid] ** P) * 0.8 \n",
    "    predict_class = []\n",
    "    correct = 0\n",
    "    sort_index = sorted(range(len(row)), key=lambda k: row[k], reverse=True)\n",
    "    for key, value in categories_mapping.items():\n",
    "        if value in sort_index[0:6]:\n",
    "            predict_class.append(key)\n",
    "            \n",
    "    total_predict.append(predict_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dev_ori['categories'] = total_predict\n",
    "df_dev_ori.head()\n",
    "df_dev_ori.to_json('./results/dev.json', orient='records', lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
